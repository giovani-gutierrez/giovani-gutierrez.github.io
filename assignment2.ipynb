{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "50fab8bb-cf5c-4bab-bcec-d37d5da1561a",
      "metadata": {
        "id": "50fab8bb-cf5c-4bab-bcec-d37d5da1561a"
      },
      "source": [
        "# Assignment 2: Recurrent neural networks\n",
        "\n",
        "## Due: by 11:59pm, Wednesday 6/7\n",
        "\n",
        "*Note: late submissions are accepted up to 5 days after the deadline for this assignment. You will receive 4 free late days to use between the two assignments of the course; once you exhaust those late days, each (partial) day that a submission is late will cap your maximum grade for the assignment by 10%. If you submit this assignment more than 5 days late, you will receive no credit.*\n",
        "\n",
        "**Before submitting**, please click the *Kernel* menu at the top of JupyterLab and select the *Restart Kernel and Run All Cells...* button, then click the red *Restart* button on the box that pops up. This ensures that your code will run on its own, without relying on information that you may have added and then removed when developing it. **Please note that some parts of the code may be time-consuming to run. You should plan accordingly so that you don't start rerunning the code too close to the deadline.**\n",
        "\n",
        "**To submit** your assignment, first make sure you have saved it, by pressing *Ctrl + S* or *Cmd + S* or clicking on the disk icon at the top of the notebook. Then [use git / GitHub Desktop](https://gauchospace.ucsb.edu/courses/mod/page/view.php?id=4033867) to *add* your completed assignment file(s) to git tracking, *commit* them to your repository with the message `Final submission`, and *push* them to GitHub. **We recommend going through the process of submitting sufficiently early that you can come to office hours to get help if needed; you can submit multiple times, and only your final submission (with the commit message `Final submission` will be graded).*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf66eb38-c691-47e8-8fc1-1237067acc86",
      "metadata": {
        "id": "cf66eb38-c691-47e8-8fc1-1237067acc86"
      },
      "source": [
        "## Please answer this question prior to submitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced8c453-8b9c-4368-8996-457773cfc7fc",
      "metadata": {
        "id": "ced8c453-8b9c-4368-8996-457773cfc7fc"
      },
      "source": [
        "<span style=\"color: red;\">**Did you consult anyone other than the TA/instructor, or any resources other than those listed on GauchoSpace, for this assignment? If so, please list them below.**</span>\n",
        "\n",
        "> It is fine to consult classmates and external resources, as long as the work you submit is your own. We would just like to know who you consulted, or what other resources you might have found helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb2862f-68fb-4145-b5db-07263fb1cb4b",
      "metadata": {
        "id": "ceb2862f-68fb-4145-b5db-07263fb1cb4b"
      },
      "source": [
        "*Double-click on this text cell to enter your response*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461819c8-5175-4a29-881e-5d8851773d3f",
      "metadata": {
        "tags": [],
        "id": "461819c8-5175-4a29-881e-5d8851773d3f"
      },
      "source": [
        "# Part 1: RNNs in PyTorch [50 pts]\n",
        "\n",
        "In this part of the assignment, you'll learn how to create recurrent neural networks in PyTorch, and you'll show your conceptual understanding by exploring and explaining key design decisions.\n",
        "\n",
        "Here are some other tutorials that explore aspects of RNNs we aren't able to fit in this assignment:  \n",
        "- [NLP From Scratch: Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)  \n",
        "- [Generating spam from a character-level RNN](https://www.cs.toronto.edu/~lczhang/aps360_20191/lec/w08/rnn.html)  \n",
        "- [NLP From Scratch: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)  \n",
        "- [Seq2seq learning with neural networks](https://github.com/bentrevett/pytorch-seq2seq/tree/master)  \n",
        "- [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)  \n",
        "\n",
        "To begin with, run the cell below to import everything you'll need. It will also create several functions and variables from Assignment 1 that we'll be reusing here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c576f0c-ef61-4bdc-9715-e0211ad8e1ff",
      "metadata": {
        "id": "8c576f0c-ef61-4bdc-9715-e0211ad8e1ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vectors\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "# ========================\n",
        "# LOADING DATA - GENERAL\n",
        "\n",
        "class TSVDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, filepath):\n",
        "        \"\"\"Loads the data from a provided filepath\"\"\"\n",
        "        self.data = list()\n",
        "        with open(filepath, encoding=\"utf-8\") as in_file:\n",
        "            for line in in_file:\n",
        "                (label, text) = line.strip().split(\"\\t\")\n",
        "                self.data.append((label, text))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns the datapoint at a given index\"\"\"\n",
        "        return self.data[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of datapoints in the dataset\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "spacy_tokenizer = get_tokenizer('spacy', language=\"en_core_web_sm\")\n",
        "tokenizer = lambda text: [token.lower() for token in spacy_tokenizer(text)]\n",
        "    \n",
        "def text_to_indices(text):\n",
        "    tokens = tokenizer(text)\n",
        "    indices = vocab(tokens)\n",
        "    return torch.tensor(indices, dtype=torch.int64)\n",
        "\n",
        "def label_to_index(label):\n",
        "    return int(label == \"pos\")\n",
        "\n",
        "def data_to_indices(data):\n",
        "    (label, text) = data\n",
        "    return (label_to_index(label), text_to_indices(text))\n",
        "\n",
        "train_data = TSVDataset(\"inputs/imdb-train.tsv\")\n",
        "test_data = TSVDataset(\"inputs/imdb-test.tsv\")\n",
        "\n",
        "\n",
        "# =================================================\n",
        "# SETTING UP THE VOCAB AND EMBEDDINGS - GENERAL\n",
        "\n",
        "def yield_tokens(data):\n",
        "    \"\"\"A generator for tokenizing text in a (label, text) pair\"\"\"\n",
        "    for _, text in data:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "tokenized_iter = yield_tokens(train_data)\n",
        "embeddings = Vectors(\"inputs/glove_6B_50_sample_train.txt\")\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# TRAINING AND TESTING - GENERAL\n",
        "\n",
        "def train(model, dataloader, optimizer, epochs=100, print_every=1,\n",
        "          validation_data=None):\n",
        "    \"\"\"Train a PyTorch model and print results periodically\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    model: torch.nn.Module; the model to be trained\n",
        "    dataloader: torch.utils.data.DataLoader; the training data\n",
        "    optimizer: the PyTorch optimizer to use for training\n",
        "    epochs: int; the number of complete cycles through the training data\n",
        "    print_every: int; print the results after this many epochs\n",
        "                 (does not print if this is None)\n",
        "    validation_data: torch.utils.data.DataLoader; the validation data\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if print_every is not None:\n",
        "        # Print initial performance\n",
        "        initial_performance = test(model, dataloader)\n",
        "        log_message = '| epoch   0 | train acc {acc:6.3f} | train loss {loss:6.3f} |'.format(**initial_performance)\n",
        "        if validation_data is not None:\n",
        "            validation_performance = test(model, validation_data)\n",
        "            log_message += ' valid acc {acc:6.3f} | valid loss {loss:6.3f} |'.format(**validation_performance)\n",
        "        print(log_message)\n",
        "        \n",
        "        # Set up trackers for printing results along the way\n",
        "        total_acc = 0\n",
        "        total_count = 0\n",
        "        current_loss = 0.0\n",
        "        minibatches_per_log = len(dataloader) * print_every\n",
        "        \n",
        "    # Tell the model that these inputs will be used for training\n",
        "    model.train()\n",
        "        \n",
        "    for epoch in range(epochs):\n",
        "        # Within each epoch, iterate over the data in mini-batches\n",
        "        # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
        "        for (label_list, *datapoint_list) in dataloader:\n",
        "            \n",
        "            # Clear out gradients accumulated from inputs in the previous mini-batch\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Run the forward pass to make predictions for the mini-batch\n",
        "            predicted_probs = model(*datapoint_list).view(-1)\n",
        "\n",
        "            # Compute the loss and send it backward through the network to get gradients\n",
        "            # Note: PyTorch averages the loss over all datapoints in the minibatch\n",
        "            loss = model.loss_function(predicted_probs, label_list.to(torch.float32))\n",
        "            loss.backward()\n",
        "            \n",
        "            # Nudge the weights\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Track performance\n",
        "            if print_every is not None: \n",
        "                total_acc += ((predicted_probs > 0.5).to(torch.int64) == label_list).sum().item()\n",
        "                total_count += label_list.size(0)\n",
        "                current_loss += loss.item()\n",
        "\n",
        "        # Log performance\n",
        "        if print_every is not None and (epoch + 1) % print_every == 0:\n",
        "            log_message = ('| epoch {:3d} | train acc {:6.3f} | train loss {:6.3f} |'\n",
        "                           .format(epoch + 1, total_acc/total_count, current_loss/minibatches_per_log))\n",
        "            if validation_data is not None:\n",
        "                validation_performance = test(model, validation_data)\n",
        "                log_message += ' valid acc {acc:6.3f} | valid loss {loss:6.3f} |'.format(**validation_performance)\n",
        "            print(log_message)\n",
        "\n",
        "            # Reset trackers after logging\n",
        "            total_acc = 0\n",
        "            total_count = 0\n",
        "            current_loss = 0.0\n",
        "            model.train()\n",
        "            \n",
        "    print(\"\\nOverall training time: {:.0f} seconds\".format(time.time() - start_time))\n",
        "            \n",
        "def test(model, dataloader):\n",
        "    \"\"\"Evaluate a PyTorch model by testing it on labeled data\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    model: torch.nn.Module; the model to be tested\n",
        "    dataloader: torch.utils.data.DataLoader; the test data\n",
        "    \"\"\"\n",
        "    # Tell the model that these inputs will be used for evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    # Set up trackers\n",
        "    total_acc = 0\n",
        "    total_count = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    with torch.no_grad(): # This can speed things up by telling PyTorch to ignore gradients\n",
        "        # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
        "        for (label_list, *datapoint_list) in dataloader:\n",
        "            # Get the model's output predictions\n",
        "            predicted_probs = model(*datapoint_list).view(-1)\n",
        "            predicted_labels = (predicted_probs > 0.5).to(torch.int64)\n",
        "            \n",
        "            # Calculate the loss and accuracy\n",
        "            loss += model.loss_function(predicted_probs, label_list.to(torch.float32)).item()\n",
        "            total_acc += (predicted_labels == label_list).sum().item()\n",
        "            total_count += label_list.size(0)\n",
        "    \n",
        "    performance = {\"acc\": total_acc/total_count, \"loss\": loss/len(dataloader)}\n",
        "    return performance\n",
        "\n",
        "\n",
        "# ==================================\n",
        "# INSPECTING A MODEL - GENERAL\n",
        "\n",
        "def display_weights(model):\n",
        "    \"\"\"Prints the weights of a model\"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        print(name.upper(), param)\n",
        "        print()\n",
        "        \n",
        "def predict_multiple(model, texts, collate_batch_fn, labels=[\"neg\", \"pos\"]):\n",
        "    \"\"\"Prints a model's predictions for a list of input texts.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    model: torch.nn.Module; a PyTorch RNN model\n",
        "    texts: list(str); a list of untokenized strings to feed as input to the model\n",
        "    collate_batch_fn: function; a function that is used to prepare (batched) data\n",
        "                      to be input into the model\n",
        "    labels: list(str); a list of the labels that correspond to the indices the\n",
        "            model will output\n",
        "    \"\"\"\n",
        "    # Tell the model not to use these inputs for training\n",
        "    model.eval()\n",
        "    \n",
        "    # Convert the input texts to indices, and get other model arguments needed\n",
        "    data = [(None, text) for text in texts]\n",
        "    (_, *model_input) = collate_batch_fn(data)\n",
        "    \n",
        "    # Feed the inputs through the model\n",
        "    with torch.no_grad():\n",
        "        probs = model(*model_input).view(-1)\n",
        "    \n",
        "    # Collate the predictions in a DataFrame\n",
        "    predictions = pd.DataFrame({\"Input text\": texts, \"Classifier probability\": probs})\n",
        "    predictions[\"Output label\"] = labels[0]\n",
        "    predictions.loc[predictions[\"Classifier probability\"] > 0.5, \"Output label\"] = \"pos\"\n",
        "    return predictions\n",
        "\n",
        "        \n",
        "# =================================\n",
        "# LOADING DATA - SPECIFIC TO BOE\n",
        "\n",
        "def collate_batch_boe(batch):\n",
        "    \"\"\"Converts a batch of data into PyTorch tensor format, and collates\n",
        "    the results by label, text, and offset, for use in a bag-of-embeddings\n",
        "    model.\n",
        "    \"\"\"\n",
        "    # Initialize lists that separate out the three components\n",
        "    label_list = list()\n",
        "    text_list = list()\n",
        "    offsets_list = [0]\n",
        "    \n",
        "    for data in batch:\n",
        "        # Convert to PyTorch format\n",
        "        (label_index, text_indices) = data_to_indices(data)\n",
        "        # Add converted data to separate component lists\n",
        "        label_list.append(label_index)\n",
        "        text_list.append(text_indices)\n",
        "        offsets_list.append(text_indices.size(0))\n",
        "        \n",
        "    # Convert everything to tensors\n",
        "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_tensor = torch.cat(text_list)\n",
        "    offsets_tensor = torch.tensor(offsets_list[:-1]).cumsum(dim=0)\n",
        "    \n",
        "    return (label_tensor, text_tensor, offsets_tensor)\n",
        "          \n",
        "train_dataloader_boe = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch_boe)\n",
        "test_dataloader_boe = DataLoader(test_data, batch_size=1000, collate_fn=collate_batch_boe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146299c6-8ee8-46c6-a992-9abe3d04fb2f",
      "metadata": {
        "tags": [],
        "id": "146299c6-8ee8-46c6-a992-9abe3d04fb2f"
      },
      "source": [
        "## Activity 1.1: Comparing recurrent and bag-of-embeddings classifiers\n",
        "\n",
        "In this activity, we'll construct sentiment analysis models for reviews in two ways: using a bag-of-embeddings approach, and using a recurrent neural network. \n",
        "\n",
        "The model that we'll use for a bag-of-embeddings approach is a simpler version of the `BagOfEmbeddingsBinaryClassifier` class from Assignment 1. This model forms a document embedding from the average of the embeddings for all the words in the document, then uses that document embedding in a feedforward neural network with a single hidden layer in order to predict whether the document has positive or negative sentiment.\n",
        "\n",
        "The code cell below defines a class that can be used as a factory for the BoE model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f887e561-186f-4253-9361-52bb8efdf96a",
      "metadata": {
        "id": "f887e561-186f-4253-9361-52bb8efdf96a"
      },
      "outputs": [],
      "source": [
        "class BagOfEmbeddingsBinaryClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab, embeddings, hidden_dim, freeze_embeddings=True):\n",
        "        super(BagOfEmbeddingsBinaryClassifier, self).__init__()\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        \n",
        "        vocab_embeddings = embeddings.get_vecs_by_tokens(self.vocab.get_itos())\n",
        "        self.embedding = nn.EmbeddingBag.from_pretrained(vocab_embeddings, freeze=freeze_embeddings, mode=\"mean\")\n",
        "        \n",
        "        # The hidden layer will go from the embeddings to a layer of hidden_dim units\n",
        "        self.hidden_layer = nn.Sequential(\n",
        "            nn.Linear(embeddings.dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # The output layer will go from the hidden layer (hidden_dim units) to a single unit\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.loss_function = nn.BCELoss()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        # The hidden layer slots in between the embeddings and the outputs\n",
        "        doc_embedding = self.embedding(text, offsets)\n",
        "        hidden = self.hidden_layer(doc_embedding)\n",
        "        output = self.output_layer(hidden)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee60ffd2-ed60-471a-a09c-803d2b881edb",
      "metadata": {
        "tags": [],
        "id": "ee60ffd2-ed60-471a-a09c-803d2b881edb"
      },
      "source": [
        "For the RNN, we'll use a unidirectional (forward) model that is as similar to the bag-of-embeddings model as possible. That means the model will take the sequence of word embeddings in the document as input, combine them into a document embedding using recurrence, and then use that document embedding in a feedforward neural network with a single hidden layer to make the prediction. We'll have all the layers be the same size as they are in the BoE model; that means the recurrent layer will be the same size as the input embeddings (so that the resultant document embedding is also the same size as the word embeddings, like it is in BoE), and the hidden layer in the classifier will be the same size as it is in BoE. \n",
        "\n",
        "The code cell below defines a class that can be used as a factory for the RNN model, using either `\"tanh\"` or `\"relu\"` as the `recurrent_activation` function. We will only use tanh (the default) in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f79167-d6a9-4b27-bf80-53ed5843cb9d",
      "metadata": {
        "id": "36f79167-d6a9-4b27-bf80-53ed5843cb9d"
      },
      "outputs": [],
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab, embeddings, recurrent_dim, hidden_dim, freeze_embeddings=True,\n",
        "                 recurrent_activation=\"tanh\", recurrent_layers=1, recurrent_bidirectional=False):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        \n",
        "        vocab_embeddings = embeddings.get_vecs_by_tokens(self.vocab.get_itos())\n",
        "        padding_idx = self.vocab.get_stoi().get(\"<pad>\")  # Get the <pad> index\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab_embeddings, freeze=freeze_embeddings, \n",
        "                                                      padding_idx=padding_idx) # Tell PyTorch that <pad> is for padding\n",
        "        \n",
        "        # The embeddings go into an RNN layer with recurrent_dim units\n",
        "        self.recurrent_layer = nn.RNN(embeddings.dim, recurrent_dim, nonlinearity=recurrent_activation,\n",
        "                                      num_layers=recurrent_layers, bidirectional=recurrent_bidirectional,\n",
        "                                      batch_first=True) # Because we'll make the mini-batch a list of sequences\n",
        "        \n",
        "        # The recurrent output creates a doc_embedding, which feeds into a of hidden_dim units\n",
        "        # We'll be concatenating the forward and backward direction of all layers\n",
        "        # from the recurrent output, so the doc_embedding will be sized accordingly\n",
        "        doc_embedding_dim = recurrent_dim * recurrent_layers * int(1 + recurrent_bidirectional)\n",
        "        self.hidden_layer = nn.Sequential(\n",
        "            nn.Linear(doc_embedding_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # The output layer will go from the hidden layer (hidden_dim units) to a single unit\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.loss_function = nn.BCELoss()\n",
        "\n",
        "    def forward(self, padded_text, seq_lengths):\n",
        "        word_embeddings = self.embedding(padded_text)\n",
        "        \n",
        "        # The sequence of word embeddings has to be packed for efficiency of the RNN\n",
        "        packed_word_embeddings = pack_padded_sequence(word_embeddings, seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "        (final_layer_all_timesteps, all_layers_final_timestep) = self.recurrent_layer(packed_word_embeddings)\n",
        "        \n",
        "        # all_layers_final_timestep contains the activations of all (stacked / bidirectional) recurrent\n",
        "        # layers at the final timestep for each sequence (taking the padding into account).\n",
        "        # For our classifier, we will stick all of these layers together (forward + backward, \n",
        "        # for each stacked layer) to use as the document embedding.\n",
        "        # all_layers_final_timestep has shape (num_layers, minibatch_size, recurrent_dim);\n",
        "        # we want something of shape (minibatch_size, num_layers * recurrent_dim),\n",
        "        # so we reorder the dimensions and then reshape to stick everything together\n",
        "        minibatch_size = all_layers_final_timestep.size(1)\n",
        "        doc_embedding = all_layers_final_timestep.permute(1, 0, 2).reshape(minibatch_size, -1)\n",
        "        \n",
        "        hidden = self.hidden_layer(doc_embedding)\n",
        "        output = self.output_layer(hidden)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a7dcec8-204a-459a-87b5-56d4bcd1a4c7",
      "metadata": {
        "tags": [],
        "id": "7a7dcec8-204a-459a-87b5-56d4bcd1a4c7"
      },
      "source": [
        "All that differs between our two models is how they make the document embedding from the word embeddings: BoE uses (mean) pooling, while RNN uses recurrence. The BoE model can therefore be thought of as a special case of the RNN model, where there is no activation function at the recurrent layer, the input weights are fixed to $I/n$ (where $I$ is the identity matrix; this divides each word embedding by the number of words in the document, then adds it to the pool at the recurrent layer), and the recurrent weights are fixed to $I$ (where $I$ is the identity matrix; this carries forward all of the information pooled from the previous timesteps)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f791f044-3d2b-40e1-b33c-0d26e75e7d03",
      "metadata": {
        "id": "f791f044-3d2b-40e1-b33c-0d26e75e7d03"
      },
      "source": [
        "**MINI-BATCHING IN AN RNN**\n",
        "\n",
        "Recall that we usually provide training samples to the model in *mini-batches*, for efficiency. All of the training samples in a mini-batch are processed together. For feedfoward NNs, the input in each training sample was a vector of the same size; but for RNNs, the input in each training sample is a *sequence* of vectors, and different sequences may have different lengths. In order to form mini-batches appropriately, we have to ensure that all of the sequences in a mini-batch are the same length.\n",
        "\n",
        "Of course, we probably don't want to actually change the length of the input sequences in our training data, by cutting long sequences short. So instead, we *pad* short sequences to make them longer. PyTorch knows that this padding is only there for technical reasons, and shouldn't count when making predictions or doing backprop.\n",
        "\n",
        "To be able to pad the sequences, we first have to add a special symbol `\"<pad>\"` to our vocabulary (like `\"<unk>\"`). The best time to do this is when loading the data, by including `\"<pad>\"` in the list of `specials` given as keyword argument to the `build_vocab_from_iterator()` function. The `specials` are represented by the first few indices in the vocab; for example, if `specials=[\"<pad>\", \"<unk>\"]`, then `\"<pad>\"` will be represented by the index 0 and `\"<unk>\"` will be represented by the index 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b372b548-dc4d-4d31-a8aa-31e22cc85eed",
      "metadata": {
        "id": "b372b548-dc4d-4d31-a8aa-31e22cc85eed"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab_from_iterator(tokenized_iter, specials=[\"<pad>\", \"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "padding_idx = vocab.get_stoi().get(\"<pad>\")\n",
        "print(\"The <pad> symbol in this vocabulary is represented by the index {}\".format(padding_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19fa5ad-ce3d-4b61-be94-10232b14f7c4",
      "metadata": {
        "id": "d19fa5ad-ce3d-4b61-be94-10232b14f7c4"
      },
      "source": [
        "We also have to tell the input (`self.embedding`) layer what the padding index is, which is accomplished through the `padding_idx` keyword argument of the  `nn.Embedding.from_pretrained()` method in the `RNNClassifier` class above.\n",
        "\n",
        "One we have a pad token in the vocab, we need to use it to pad out the mini-batch of sequences, and we need to tell PyTorch to ignore this padding when making predictions or doing backprop (which is accomplished by *packing* the sequence). PyTorch provides two functions to help with this: `pad_sequence()` and `pack_padded_sequence()` (both in `torch.nn.utils.rnn`). We use `pad_sequence()` in our custom `collate_batch()` function that we use to get mini-batches from a DataLoader, and `pack_padded_sequence()` in the `forward()` method of the `RNNClassifier` (because it is the *embeddings* that need to be packed, and they are looked up inside `forward()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283f6281-e4e7-4790-bed2-35e83a55e981",
      "metadata": {
        "id": "283f6281-e4e7-4790-bed2-35e83a55e981"
      },
      "outputs": [],
      "source": [
        "def collate_batch_rnn(batch):\n",
        "    \"\"\"Converts a batch of sequence data into padded and packed PyTorch \n",
        "    tensor format, and collates the results by label, text, and sequence\n",
        "    length, for use in a RNN model.\n",
        "    \"\"\"\n",
        "    # Initialize lists that separate out the two components\n",
        "    label_list = list()\n",
        "    text_list = list()\n",
        "    seq_lengths = list()\n",
        "    \n",
        "    for data in batch:\n",
        "        # Convert to PyTorch format\n",
        "        (label_index, text_indices) = data_to_indices(data)\n",
        "        # Add converted data to separate component lists\n",
        "        label_list.append(label_index)\n",
        "        text_list.append(text_indices)\n",
        "        seq_lengths.append(len(text_indices))\n",
        "    \n",
        "    # Convert to mini-batch tensors\n",
        "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_tensor = pad_sequence(text_list, batch_first=True, padding_value=padding_idx)\n",
        "    \n",
        "    return (label_tensor, text_tensor, seq_lengths)\n",
        "          \n",
        "train_dataloader_rnn = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch_rnn)\n",
        "test_dataloader_rnn = DataLoader(test_data, batch_size=1000, collate_fn=collate_batch_rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260068e7-f006-45f3-8682-b9bd55ca5d8c",
      "metadata": {
        "id": "260068e7-f006-45f3-8682-b9bd55ca5d8c"
      },
      "source": [
        "Note that padding complicates matters somewhat: we want the classifier to use the recurrent layer activations from the last non-padding timestep of each sequence, but they may all be in different positions! This is the reason for the extra steps taken to get `doc_embedding` inside `RNNClassifier.foward()`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3cf362b-641a-4ed2-8593-5915d7652529",
      "metadata": {
        "id": "c3cf362b-641a-4ed2-8593-5915d7652529"
      },
      "source": [
        "---\n",
        "---\n",
        "### Questions 1.1.1 - 1.1.2 \n",
        "\n",
        "The code cell below shows an example of padding. Run it, then answer the following question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efcc49b4-756b-4ef4-ad95-68806e5cf422",
      "metadata": {
        "id": "efcc49b4-756b-4ef4-ad95-68806e5cf422"
      },
      "outputs": [],
      "source": [
        "# Demonstration of padding\n",
        "texts = [\"I hated it\", \"it was quite terrible\", \"i really REALLY loved it\"]\n",
        "print(\"texts:\\n{}\\n\".format(texts))\n",
        "\n",
        "text_indices = [text_to_indices(text) for text in texts]\n",
        "print(\"Converted to indices:\\n{}\\n\".format(text_indices))\n",
        "\n",
        "padded = pad_sequence(text_indices, batch_first=True, padding_value=padding_idx)\n",
        "print(\"Padded:\\n{}\".format(padded))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c5dbed-7fdc-4c5f-a54a-7c490bfeb228",
      "metadata": {
        "id": "30c5dbed-7fdc-4c5f-a54a-7c490bfeb228"
      },
      "source": [
        "**QUESTION 1.1.1. [1 point]**  \n",
        "How do the padded sequences relate to the original sequences? What are the 0s in the padded tensor, why are they where they are, and why are there as many as there are?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "337039a7-ede3-4f74-93e5-f905297e1138",
      "metadata": {
        "id": "337039a7-ede3-4f74-93e5-f905297e1138"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5730e8-cce4-451c-ba1c-b6d3c1465613",
      "metadata": {
        "id": "4e5730e8-cce4-451c-ba1c-b6d3c1465613"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee0d153-573a-4157-b57f-f7d12b05c0bb",
      "metadata": {
        "id": "6ee0d153-573a-4157-b57f-f7d12b05c0bb"
      },
      "source": [
        "The code cell below demonstrates the importance of telling PyTorch to ignore padding, through use of *packing*. It defines a simple RNN in which the recurrent layer activation at each timestep increases by $1 + x_t$, where $x_t$ is the input value at timestep $t$. Thus, the recurrent layer activation at timestep $t$ is equal to $t$ plus the sum of the input values at all timesteps up to and including the current timestep.\n",
        "\n",
        "Run the code cell, then answer the following question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7041fc4f-44e4-4cbf-9d3b-6965c6b80cce",
      "metadata": {
        "id": "7041fc4f-44e4-4cbf-9d3b-6965c6b80cce"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad(): # This tells PyTorch we aren't going to be doing backprop\n",
        "    \n",
        "    # Define simple embedding and recurrent layers.\n",
        "    # The embedding layer just contains the index of the input\n",
        "    # The recurrent layer has a bias of 1 and all weights fixed to 1,\n",
        "    # so it adds together all of the inputs up to the current timestep,\n",
        "    # plus the number of timesteps since the beginning of the sequence\n",
        "    embedding_layer = nn.Embedding.from_pretrained(torch.arange(5000, dtype=torch.float32).view(-1, 1), 1, padding_idx=padding_idx)\n",
        "    recurrent_layer = nn.RNN(1, 1, batch_first=True, nonlinearity=\"relu\")\n",
        "    recurrent_layer.bias_ih_l0[0] = 0.0\n",
        "    recurrent_layer.bias_hh_l0[0] = 1.0\n",
        "    recurrent_layer.weight_ih_l0[0, 0] = 1.0\n",
        "    recurrent_layer.weight_hh_l0[0, 0] = 1.0\n",
        "\n",
        "    # Get the word embeddings for the padded sequence\n",
        "    word_embeddings = embedding_layer(padded)\n",
        "\n",
        "    # With packing: pack the word embeddings, \n",
        "    # run the recurrent layer on the packed padded embeddings, \n",
        "    # and then unpack the results\n",
        "    packed_word_embeddings = pack_padded_sequence(word_embeddings, [3, 4, 5], batch_first=True, enforce_sorted=False)\n",
        "    (final_layer_all_timesteps, all_layers_final_timestep) = recurrent_layer(packed_word_embeddings)\n",
        "    final_layer_all_timesteps = pad_packed_sequence(final_layer_all_timesteps, batch_first=True, padding_value=padding_idx)\n",
        "\n",
        "    print(\"===========================\")\n",
        "    print(\"WITH PACKING\")\n",
        "    print(\"---------------------------\")\n",
        "    print(\"Recurrent layer activation at all timesteps, for each sequence:\")\n",
        "    print(final_layer_all_timesteps[0].view(3, -1))\n",
        "    print(\"\\nWhat PyTorch gets as the recurrent layer activation at the \\\"final\\\" timestep for each sequence:\")\n",
        "    for (seqnum, value) in enumerate(list(all_layers_final_timestep.view(-1))):\n",
        "        print(\"Sequence {}: {}\".format(seqnum + 1, int(value)))\n",
        "\n",
        "    # Without packing: run the recurrent layer on the padded embeddings directly\n",
        "    (final_layer_all_timesteps, all_layers_final_timestep) = recurrent_layer(word_embeddings)\n",
        "\n",
        "    print(\"\\n===========================\")\n",
        "    print(\"WITHOUT PACKING\")\n",
        "    print(\"---------------------------\")\n",
        "    print(\"Recurrent layer activation at all timesteps, for each sequence:\")\n",
        "    print(final_layer_all_timesteps.view(3, -1))\n",
        "    print(\"\\nWhat PyTorch gets as the recurrent layer activation at the \\\"final\\\" timestep for each sequence:\")\n",
        "    for (seqnum, value) in enumerate(list(all_layers_final_timestep.view(-1))):\n",
        "        print(\"Sequence {}: {}\".format(seqnum + 1, int(value)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aada0476-26cb-450e-aaff-f856ef715737",
      "metadata": {
        "id": "aada0476-26cb-450e-aaff-f856ef715737"
      },
      "source": [
        "**QUESTION 1.1.2. [2 points]**  \n",
        "Which timestep in a sequence is treated as \"final\" with packing, and which timestep is treated as \"final\" without packing? What has to be true about a sequence in order for these two definitions of the \"final\" timestep to be the same? In sequences where they are *not* the same, how/why does using padding without packing make the value of the recurrent layer activation at the \"final\" timestep misleading?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee3432d-6891-4e40-ae08-8f413d077290",
      "metadata": {
        "id": "dee3432d-6891-4e40-ae08-8f413d077290"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f7be1d-758f-4f6a-8ac1-0eae183a0798",
      "metadata": {
        "id": "a4f7be1d-758f-4f6a-8ac1-0eae183a0798"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "Now that we have padded our data, we can use it to train the RNN. Training looks exactly the same as it does for other NN models we have seen before:  \n",
        "\n",
        "1. set the seed, in order to ensure results are reproducible;  \n",
        "2. create the model as an instance of the relevant class;  \n",
        "3. define an optimizer for the model, which will be used to nudge the weights during training;  \n",
        "4. train the model for a specified number of epochs.\n",
        "\n",
        "Since calculating the gradients in RNNs involves many steps, the `SGD` optimizer that we used in the past is typically too simple to enable good performance. We will still use it here, but we will explore alternatives later in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "416d505a-d666-474f-8f7f-6406030520c0",
      "metadata": {
        "id": "416d505a-d666-474f-8f7f-6406030520c0"
      },
      "source": [
        "---\n",
        "---\n",
        "### Questions 1.1.3 - 1.1.10 \n",
        "\n",
        "The code cell below trains a RNN classifier for sentiment analysis on the movie reviews dataset we used in Assignment 1, for which the number of units in the recurrent layer has been set to the size of an embedding (third argument `embeddings.dim`). Run the code cell to train the model, and then answer the following questions.\n",
        "\n",
        "*Note: it may take a few minutes to train the model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5f9239-2bb0-4fcf-a108-4c05721c53ee",
      "metadata": {
        "id": "1a5f9239-2bb0-4fcf-a108-4c05721c53ee"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "rnn_classifier = RNNClassifier(vocab, embeddings, embeddings.dim, 20)\n",
        "optimizer = optim.SGD(rnn_classifier.parameters(), lr=0.01)\n",
        "train(rnn_classifier, train_dataloader_rnn, optimizer, epochs=100, print_every=5, validation_data=test_dataloader_rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cbb5e4-f893-4fa3-a158-912dd9f86044",
      "metadata": {
        "id": "09cbb5e4-f893-4fa3-a158-912dd9f86044"
      },
      "source": [
        "**QUESTION 1.1.3. [2 points]**  \n",
        "Focus first on the training accuracy and training loss, presented in the second and third columns of the table above. How do these numbers change as training epochs pass? What does that mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5aaa650-d3d7-4c01-b41e-d4bb9a78d515",
      "metadata": {
        "id": "d5aaa650-d3d7-4c01-b41e-d4bb9a78d515"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa7be431-d98e-40a1-bba8-617982c09fcf",
      "metadata": {
        "id": "aa7be431-d98e-40a1-bba8-617982c09fcf"
      },
      "source": [
        "**QUESTION 1.1.4. [2 points]**  \n",
        "Now focus on the validation accuracy and validation loss, presented in the fourth and fifth columns. How do these numbers change as training epochs pass? How does that compare to what happens with the training accuracy and training loss? What does that suggest?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a643162-2fda-4033-922a-bbd1e0255f7c",
      "metadata": {
        "id": "9a643162-2fda-4033-922a-bbd1e0255f7c"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37587b6-d3e4-4945-915b-70c406b7e753",
      "metadata": {
        "id": "a37587b6-d3e4-4945-915b-70c406b7e753"
      },
      "source": [
        "---\n",
        "\n",
        "To put the RNN model in context, it is helpful to compare it against a bag-of-embeddings classifier like the one we saw in Assignment 1. The following code cell creates and trains such a model. Run it, then answer the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228ae2ef-640d-4213-9d7c-b9cc3f120bc8",
      "metadata": {
        "id": "228ae2ef-640d-4213-9d7c-b9cc3f120bc8"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "boe_classifier = BagOfEmbeddingsBinaryClassifier(vocab, embeddings, 20)\n",
        "optimizer = optim.SGD(boe_classifier.parameters(), lr=0.01)\n",
        "train(boe_classifier, train_dataloader_boe, optimizer, epochs=100, print_every=5, validation_data=test_dataloader_boe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3daa595e-314c-4e04-88d0-5469685bb3e7",
      "metadata": {
        "id": "3daa595e-314c-4e04-88d0-5469685bb3e7"
      },
      "source": [
        "**QUESTION 1.1.5. [2 points]**  \n",
        "How does performance on the training set differ between the RNN and BoE models? What does that mean for the potential to capture complex patterns in the RNN model as compared to the BoE model, and where does that potential come from?  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b0e0d5-5ff5-49e4-bdba-632f782e034d",
      "metadata": {
        "id": "e9b0e0d5-5ff5-49e4-bdba-632f782e034d"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f35eb97-a4fc-4de0-8c82-8c00320ad7cd",
      "metadata": {
        "id": "6f35eb97-a4fc-4de0-8c82-8c00320ad7cd"
      },
      "source": [
        "**QUESTION 1.1.6. [2 points]**  \n",
        "How does performance on the validation set differ between the RNN and BoE models? What does that mean for the risk of overfitting in the RNN model as compared to the BoE model, and where does that risk come from?  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a798d60-f9f0-4ba1-8fe0-de39eb338057",
      "metadata": {
        "id": "4a798d60-f9f0-4ba1-8fe0-de39eb338057"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20807566-bed8-42d4-ba0d-1cadeb39ba3d",
      "metadata": {
        "id": "20807566-bed8-42d4-ba0d-1cadeb39ba3d"
      },
      "source": [
        "**QUESTION 1.1.7. [1 point]**  \n",
        "How does training time differ between the RNN and BoE models, and why?  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5264ec4-d272-419a-98d9-13b3f125d922",
      "metadata": {
        "id": "c5264ec4-d272-419a-98d9-13b3f125d922"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "368c81c3-890f-4d5f-9f38-73586610e2e4",
      "metadata": {
        "id": "368c81c3-890f-4d5f-9f38-73586610e2e4"
      },
      "source": [
        "---\n",
        "\n",
        "So far, it might not be entirely clear why we might choose an RNN over a BoE model. To elucidate this point, it can be helpful to compare predictions of the two models on the same constructed data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b1aabe6-f2af-480c-a887-09631e9f49b1",
      "metadata": {
        "id": "0b1aabe6-f2af-480c-a887-09631e9f49b1"
      },
      "source": [
        "**QUESTION 1.1.8. [1 point]**  \n",
        "The code cell below generates predictions from the RNN model for two texts that swap the positions of two words, and the code cell after that generates predictions from the BoE model for the same texts. Run the code cells to see the predicted classifications. Why do the two texts get the same prediction under the BoE model but different predictions under the RNN model? Which model seems to be best?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490698d9-de82-41dc-902d-c6a71ee5e93f",
      "metadata": {
        "id": "490698d9-de82-41dc-902d-c6a71ee5e93f"
      },
      "outputs": [],
      "source": [
        "print(\"\\nRNN predictions:\")\n",
        "predict_multiple(rnn_classifier, [\"I loved it at first, but by the end I hated it\", \"I hated it at first, but by the end I loved it\"], collate_batch_rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50f8206-ce2c-470f-af5e-27c5be2a8b76",
      "metadata": {
        "id": "f50f8206-ce2c-470f-af5e-27c5be2a8b76"
      },
      "outputs": [],
      "source": [
        "print(\"\\nBoE predictions:\")\n",
        "predict_multiple(boe_classifier, [\"I hated it at first, but by the end I loved it\", \"I loved it at first, but by the end I hated it\"], collate_batch_boe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f64db8-ad90-4e8f-9f1e-1e24e2b8c953",
      "metadata": {
        "id": "16f64db8-ad90-4e8f-9f1e-1e24e2b8c953"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe7156e-b349-4899-b0af-057d233e7bfa",
      "metadata": {
        "id": "afe7156e-b349-4899-b0af-057d233e7bfa"
      },
      "source": [
        "---\n",
        "\n",
        "Exploring the effects of changing the texts on the RNN's predictions can also help us understand what it might be doing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe80611e-2cf2-4895-b53d-09004d221cc8",
      "metadata": {
        "id": "fe80611e-2cf2-4895-b53d-09004d221cc8"
      },
      "source": [
        "**QUESTION 1.1.9. [2 points]**  \n",
        "The code cell below generates predictions from the RNN model for two texts that differ minimally from the second text you examined above, by adding an intensifier (\"really\") at different positions. Does adding the intensifier early or late affect the classification probability more? Does this agree with your expectations about the effect of the intensifier on sentiment? Suggest a technical reason why the model might show this difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019ae78e-eda9-44a5-9165-781bb3c7f51e",
      "metadata": {
        "id": "019ae78e-eda9-44a5-9165-781bb3c7f51e"
      },
      "outputs": [],
      "source": [
        "predict_multiple(rnn_classifier, [\"I really loved it at first, but by the end I hated it\", \"I loved it at first, but by the end I really hated it\"], collate_batch_rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5eae703-87ba-4da7-b03c-4cd1821b05fe",
      "metadata": {
        "id": "e5eae703-87ba-4da7-b03c-4cd1821b05fe"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc6b2544-19db-4ad0-86f6-0ff063542c41",
      "metadata": {
        "id": "fc6b2544-19db-4ad0-86f6-0ff063542c41"
      },
      "source": [
        "**QUESTION 1.1.10. [2 points]**  \n",
        "The code cell below generates a prediction from the RNN model for a text that is similar to the ones you examined above, but with a different ordering of information. Does the prediction agree with your expectations, and with the models predictions for previous sentences? Suggest a reason why or why not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614904b8-c031-4e0c-96ff-2e0620a15e5c",
      "metadata": {
        "id": "614904b8-c031-4e0c-96ff-2e0620a15e5c"
      },
      "outputs": [],
      "source": [
        "predict_multiple(rnn_classifier, [\"I hated it in the end, even though I loved it at first\"], collate_batch_rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a31ec0f5-8d1e-4528-a06d-4f91524035b7",
      "metadata": {
        "id": "a31ec0f5-8d1e-4528-a06d-4f91524035b7"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279da1a1-3ac3-497b-9143-e327d0dce0f5",
      "metadata": {
        "id": "279da1a1-3ac3-497b-9143-e327d0dce0f5"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83592eee-8e0f-406f-b4f0-f7a1b259ba6a",
      "metadata": {
        "tags": [],
        "id": "83592eee-8e0f-406f-b4f0-f7a1b259ba6a"
      },
      "source": [
        "## Activity 1.2: Learning about words in RNNs\n",
        "\n",
        "We saw in Assignment 1 that we could improve the performance of a bag-of-embeddings classifier by allowing it to continue to learn about the meanings of words, by *fine-tuning* the embeddings. We can allow the embeddings to be fine-tuned in an RNN in the same way, by setting `freeze=False` when we load the embeddings inside the model. The `RNNClassifier` class we have provided has an argument `freeze_embeddings` that controls this setting. In the model we trained before, `freeze_embeddings` was set to `True` (the default value), which means that the embeddings were not fine-tuned; let's see what happens when we set it to to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc259ac8-6324-4669-9dc1-eb0b2a6ce134",
      "metadata": {
        "id": "dc259ac8-6324-4669-9dc1-eb0b2a6ce134"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "unfrozen_rnn_classifier = RNNClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False)\n",
        "optimizer = optim.SGD(unfrozen_rnn_classifier.parameters(), lr=0.01)\n",
        "train(unfrozen_rnn_classifier, train_dataloader_rnn, optimizer, epochs=100, print_every=5, validation_data=test_dataloader_rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3949341-8db6-4a9c-9683-3175963c26a0",
      "metadata": {
        "id": "b3949341-8db6-4a9c-9683-3175963c26a0"
      },
      "source": [
        "---\n",
        "---\n",
        "### Questions 1.2.1 - 1.2.2\n",
        "\n",
        "For these questions, you should compare the results of training the RNN model that fine-tunes embeddings (above) with the model that leaves embeddings frozen (which you trained for [Question 1.1.3](#Questions-1.1.3---1.1.10)).\n",
        "\n",
        "**QUESTION 1.2.1. [2 points]**  \n",
        "Look at the training and validation accuracies and losses in the last few epochs of training, and at the training time for the model. In what ways (if any) does it seem like allowing the model to fine-tune the embeddings has helped its performance? In what ways (if any) does it seem like fine-tuning has hurt performance?  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fde42a-d719-429d-bc80-965c8505fb72",
      "metadata": {
        "id": "59fde42a-d719-429d-bc80-965c8505fb72"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc245765-694f-4fe5-917e-60f4fa9fb033",
      "metadata": {
        "id": "dc245765-694f-4fe5-917e-60f4fa9fb033"
      },
      "source": [
        "**QUESTION 1.2.2. [2 points]**  \n",
        "At which epochs are the gains in performance most noticeable? Would you say that fine-tuning the embeddings gave a rapid improvement (i.e., even from the first few epochs) or a slow improvement (i.e., it took a while for it to really show up)? Suggest a reason why that might be the case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f59875-bd0a-44ea-ac35-a52fc28d9d97",
      "metadata": {
        "id": "a9f59875-bd0a-44ea-ac35-a52fc28d9d97"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b3edeb-d49c-4b81-a1bf-5001f25efd4e",
      "metadata": {
        "id": "47b3edeb-d49c-4b81-a1bf-5001f25efd4e"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "One way to examine the influence of allowing embeddings to be fine-tuned is to see how specific embeddings change over the course of training. For the next part of the activity, we will add new tokens (`\"<newneg>\"` and `\"<newpos>\"`) to all negative and positive (respectively) training and validation samples. These new tokens will start with embeddings composed entirely of zeros, meaning that they are assessed to have \"no meaning\" by the model. We will explore how these embeddings change after a few epochs of training, as the model learns that `\"<newneg>\"` is distinctive of negative reviews and `\"<newpos>\"` is distinctive of positive reviews.\n",
        "\n",
        "In particular, we will create *three versions* of the data, based on inserting these new tokens at different positions: one version which always inserts it at the *start* of a text sequence; one version which always inserts it in the *middle* of a text sequence; and one version which always inserts it at the *end* of a text sequence. We will compare the rates at which the model learns about the new tokens in these different positions.\n",
        "\n",
        "Run the code cell below to create the new versions of the data, and to create a key function that we'll use to explore the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08de35d9-5180-4cc8-bfb8-401cf0dc3cb0",
      "metadata": {
        "id": "08de35d9-5180-4cc8-bfb8-401cf0dc3cb0"
      },
      "outputs": [],
      "source": [
        "if \"<newneg>\" not in vocab:\n",
        "    vocab.append_token(\"<newneg>\")\n",
        "    vocab.append_token(\"<newpos>\")\n",
        "\n",
        "def collate_batch_rnn_newwords(batch, add_at=\"start\"):\n",
        "    \"\"\"Converts a batch of sequence data into padded and packed PyTorch \n",
        "    tensor format, and collates the results by label, text, and sequence\n",
        "    length, for use in a RNN model.\n",
        "    Adds a new token by class (<newpos> or <newneg>) at the start, middle,\n",
        "    or end of each text, as indicated by add_at\n",
        "    \"\"\"\n",
        "    # Initialize lists that separate out the two components\n",
        "    label_list = list()\n",
        "    text_list = list()\n",
        "    seq_lengths = list()\n",
        "    \n",
        "    for (label, text) in batch:\n",
        "        # Get index of word to add\n",
        "        new_word = \"<new{}>\".format(label)\n",
        "        new_index = torch.tensor(vocab([new_word]), dtype=torch.int64)\n",
        "        # Convert to PyTorch format\n",
        "        (label_index, text_indices) = data_to_indices((label, text))\n",
        "        # Add the new word index at the relevant spot\n",
        "        if add_at == \"start\":\n",
        "            left_part = torch.tensor([], dtype=torch.int64)\n",
        "            right_part = text_indices\n",
        "        elif add_at == \"end\":\n",
        "            left_part = text_indices\n",
        "            right_part = torch.tensor([], dtype=torch.int64)\n",
        "        elif add_at == \"middle\":\n",
        "            middle_index = len(text_indices) // 2\n",
        "            left_part = text_indices[:middle_index]\n",
        "            right_part = text_indices[middle_index:]\n",
        "        text_indices = torch.cat([left_part, new_index, right_part])\n",
        "        # Add converted data to separate component lists\n",
        "        label_list.append(label_index)\n",
        "        text_list.append(text_indices)\n",
        "        seq_lengths.append(len(text_indices))\n",
        "    \n",
        "    # Convert to mini-batch tensors\n",
        "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_tensor = pad_sequence(text_list, batch_first=True, padding_value=padding_idx)\n",
        "    \n",
        "    return (label_tensor, text_tensor, seq_lengths)\n",
        "          \n",
        "train_dataloader_newstart = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=lambda batch: collate_batch_rnn_newwords(batch, \"start\"))\n",
        "train_dataloader_newmiddle = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=lambda batch: collate_batch_rnn_newwords(batch, \"middle\"))\n",
        "train_dataloader_newend = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=lambda batch: collate_batch_rnn_newwords(batch, \"end\"))\n",
        "\n",
        "test_dataloader_newstart = DataLoader(test_data, batch_size=8, shuffle=True, collate_fn=lambda batch: collate_batch_rnn_newwords(batch, \"start\"))\n",
        "test_dataloader_newmiddle = DataLoader(test_data, batch_size=8, shuffle=True, collate_fn=lambda batch: collate_batch_rnn_newwords(batch, \"middle\"))\n",
        "test_dataloader_newend = DataLoader(test_data, batch_size=8, shuffle=True, collate_fn=lambda batch: collate_batch_rnn_newwords(batch, \"end\"))\n",
        "\n",
        "def newword_similarities(model):\n",
        "    with torch.no_grad():\n",
        "        word_embeddings = model.get_parameter(\"embedding.weight\")\n",
        "        embedding_similarities = pd.DataFrame({\n",
        "            \"word1\": [\"<newneg>\", \"<newneg>\", \"<newneg>\", \"<newpos>\", \"<newpos>\"],\n",
        "            \"word2\": [\"<newpos>\", \"boring\", \"outstanding\", \"boring\", \"outstanding\"]\n",
        "        })\n",
        "        embedding_similarities[\"similarity\"] = (\n",
        "            embedding_similarities.apply(lambda row: \n",
        "                                         float(\n",
        "                                             torch.nn.functional.cosine_similarity(\n",
        "                                                 word_embeddings[vocab([row[\"word1\"]])[0]],\n",
        "                                                 word_embeddings[vocab([row[\"word2\"]])[0]], \n",
        "                                                 dim=0)\n",
        "                                         ), axis=1))\n",
        "    return embedding_similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08549561-60f3-40f6-a962-3bf261ef68d0",
      "metadata": {
        "id": "08549561-60f3-40f6-a962-3bf261ef68d0"
      },
      "source": [
        "For comparison, here is what training looks like when a new token is added to the start of each sequence, but we do not allow the embeddings to be fine-tuned. You can think of this like a model that *doesn't* add any new tokens to the sequence, but rather initializes the recurrent layer to the same non-zero value for each sequence (whereas the regular RNN initializes it to zero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3e24a4-c88f-4f79-bc32-b1ebe8e6906a",
      "metadata": {
        "id": "ee3e24a4-c88f-4f79-bc32-b1ebe8e6906a"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "rnn_classifier_newstart_baseline = RNNClassifier(vocab, embeddings, embeddings.dim, 20)\n",
        "optimizer = optim.SGD(rnn_classifier_newstart_baseline.parameters(), lr=0.01)\n",
        "train(rnn_classifier_newstart_baseline, train_dataloader_newstart, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bde8c83-487a-4b3b-b24a-ba833d20a7d3",
      "metadata": {
        "id": "9bde8c83-487a-4b3b-b24a-ba833d20a7d3"
      },
      "source": [
        "---\n",
        "---\n",
        "### Questions 1.2.3 - 1.2.11\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff69589b-4fd4-47b3-957c-ce20eaac7edc",
      "metadata": {
        "id": "ff69589b-4fd4-47b3-957c-ce20eaac7edc"
      },
      "source": [
        "We'll start by looking at what happens when we put the new tokens at the *start* of each text sequence and allow fine-tuning of embeddings.\n",
        "\n",
        "Run the code cell below to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ae01e5-67ef-414c-a500-6ba728e0a28f",
      "metadata": {
        "id": "17ae01e5-67ef-414c-a500-6ba728e0a28f"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "rnn_classifier_newstart = RNNClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False)\n",
        "optimizer = optim.SGD(rnn_classifier_newstart.parameters(), lr=0.01)\n",
        "train(rnn_classifier_newstart, train_dataloader_newstart, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a395c54-a0e2-419a-ac98-c989bf634944",
      "metadata": {
        "id": "2a395c54-a0e2-419a-ac98-c989bf634944"
      },
      "source": [
        "**QUESTION 1.2.3. [2 points]**  \n",
        "Compare the training performance to what you saw just above, when the same model *did not* allow fine-tuning of embeddings. Have the training and validation accuracies and losses changed much? Why / why not? How about the training time? Why / why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0efdfbb1-ff37-4ddf-b59f-59a50636527c",
      "metadata": {
        "id": "0efdfbb1-ff37-4ddf-b59f-59a50636527c"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6e5608-d82c-4679-af3a-6308fc8ef23c",
      "metadata": {
        "id": "cd6e5608-d82c-4679-af3a-6308fc8ef23c"
      },
      "source": [
        "Run the following code cell to generate a report of the cosine-similarity between the fine-tuned `\"<newneg>\"` and `\"<newpos>\"` embeddings, as well as the cosine-similarity of each with two words that are distinctive of negative and positive reviews (`\"boring\"` and `\"outstanding\"`, respectively)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf2ae851-71ec-4474-873c-58861e11cb9e",
      "metadata": {
        "id": "bf2ae851-71ec-4474-873c-58861e11cb9e"
      },
      "outputs": [],
      "source": [
        "newword_similarities(rnn_classifier_newstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d813a22-a2de-4370-849d-3d87cf27db9e",
      "metadata": {
        "id": "5d813a22-a2de-4370-849d-3d87cf27db9e"
      },
      "source": [
        "**QUESTION 1.2.4. [2 points]**  \n",
        "Before training, each of these cosine-similarities were 0, because the new embeddings were initialized to contain all zeros. What do you notice about the cosine-similarity of `\"<newneg>\"` and `\"<newpos>\"` now: is it positive or negative, and how big is it? What does that mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee6672d-769b-4f30-a272-e628c5a4d2e1",
      "metadata": {
        "id": "eee6672d-769b-4f30-a272-e628c5a4d2e1"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6273ae73-9425-408f-ae79-78c62695f380",
      "metadata": {
        "id": "6273ae73-9425-408f-ae79-78c62695f380"
      },
      "source": [
        "**QUESTION 1.2.5. [2 points]**  \n",
        "What about the cosine-similarities between each new token and the two reference words? Do they make sense? What does that suggest about what the model has learned about these new tokens?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298a3501-582d-4e25-8f08-8ebe3d9f121c",
      "metadata": {
        "id": "298a3501-582d-4e25-8f08-8ebe3d9f121c"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73cfb7b7-3493-4126-a392-1f4113995bba",
      "metadata": {
        "id": "73cfb7b7-3493-4126-a392-1f4113995bba"
      },
      "source": [
        "---\n",
        "Now, for comparison, let's look at what happens when we put the new tokens at the *end* of each sequence and allow them to be fine-tuned.\n",
        "\n",
        "Run the code cell below to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250136cf-3d44-4999-848d-490d311f177a",
      "metadata": {
        "id": "250136cf-3d44-4999-848d-490d311f177a"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "rnn_classifier_newend = RNNClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False)\n",
        "optimizer = optim.SGD(rnn_classifier_newend.parameters(), lr=0.01)\n",
        "train(rnn_classifier_newend, train_dataloader_newend, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newend)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9576dc2-f177-429a-8be6-96a17dd3a73f",
      "metadata": {
        "id": "c9576dc2-f177-429a-8be6-96a17dd3a73f"
      },
      "source": [
        "**QUESTION 1.2.6. [1 point]**  \n",
        "How much have the training and validation accuracies and losses changed now? Why is that? Suggest a reason why this is so different to the previous case.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed41f2a-4b45-44a0-a6d7-fd4e68e84300",
      "metadata": {
        "id": "eed41f2a-4b45-44a0-a6d7-fd4e68e84300"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77428549-bf0b-43b7-bd59-dd441d40242b",
      "metadata": {
        "id": "77428549-bf0b-43b7-bd59-dd441d40242b"
      },
      "source": [
        "It also helps to examine the embeddings for this model. Run the code cell below to generate the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de10f7e-672e-4415-88b1-f6b959015402",
      "metadata": {
        "id": "8de10f7e-672e-4415-88b1-f6b959015402"
      },
      "outputs": [],
      "source": [
        "newword_similarities(rnn_classifier_newend)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64aea26e-d8a7-4331-9d40-eccf141e991e",
      "metadata": {
        "id": "64aea26e-d8a7-4331-9d40-eccf141e991e"
      },
      "source": [
        "**QUESTION 1.2.7. [1 point]**  \n",
        "What do you notice about the cosine-similarity of `\"<newneg>\"` and `\"<newpos>\"` now, and what does that mean? Suggest a reason why it is so different to what you saw in the previous case.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "248ed383-743f-45a0-9731-0e6ce6546101",
      "metadata": {
        "id": "248ed383-743f-45a0-9731-0e6ce6546101"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f621aa2b-a626-4d6c-a6b0-169d1d2492cf",
      "metadata": {
        "id": "f621aa2b-a626-4d6c-a6b0-169d1d2492cf"
      },
      "source": [
        "**QUESTION 1.2.8. [1 point]**  \n",
        "What about the cosine-similarities between each new token and the two reference words? Do they make sense now? What does that suggest about what the model has learned about these new tokens?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad7f00a-d0c4-4b8c-849a-3f6e0cc4f601",
      "metadata": {
        "id": "bad7f00a-d0c4-4b8c-849a-3f6e0cc4f601"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32822f9-50d6-4295-bb4b-dce4224611e8",
      "metadata": {
        "id": "d32822f9-50d6-4295-bb4b-dce4224611e8"
      },
      "source": [
        "---\n",
        "One of the things that we have to be very aware of in RNNs - especially when fine-tuning embeddings - is *gradients*. We have to be on the lookout for *vanishing gradients* and *exploding gradients*.\n",
        "\n",
        "The code cell below prints the gradients of the embeddings for the new tokens when they are inserted at the start, middle, and end of each sequence. Run the code cell to see the gradients.\n",
        "\n",
        "(Note that gradients come from considering the *loss*, which can only be measured over a set of inputs. We have calculated these gradients over the inputs in the validation set.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a38d1664-83eb-4de9-a178-56b160bef068",
      "metadata": {
        "id": "a38d1664-83eb-4de9-a178-56b160bef068"
      },
      "outputs": [],
      "source": [
        "def view_untrained_gradients(word_position, model_class, model_args, model_kwargs):\n",
        "    torch.manual_seed(1)\n",
        "    untrained_classifier = model_class(*model_args, **model_kwargs)\n",
        "\n",
        "    # Provide the whole validation set to get gradients\n",
        "    for (label_list, *datapoint_list) in eval(\"test_dataloader_new{}\".format(word_position)):\n",
        "        # Run the forward pass to make predictions for the mini-batch\n",
        "        predicted_probs = untrained_classifier(*datapoint_list).view(-1)\n",
        "        # Compute the loss and send it backward through the network to get gradients\n",
        "        # Note: PyTorch averages the loss over all datapoints in the minibatch\n",
        "        loss = untrained_classifier.loss_function(predicted_probs, label_list.to(torch.float32))\n",
        "        loss.backward()\n",
        "\n",
        "    # Now look at the gradients\n",
        "    word_embeddings = untrained_classifier.get_parameter(\"embedding.weight\")\n",
        "    (newneg_grad, newpos_grad) = word_embeddings.grad[-2:]\n",
        "    print(\"Gradient for <newneg> inserted at {}:\\n{}\".format(word_position, newneg_grad))\n",
        "    print(\"\\nGradient for <newpos> inserted at {}:\\n{}\".format(word_position, newpos_grad))\n",
        "\n",
        "for word_position in [\"start\", \"middle\", \"end\"]:\n",
        "    view_untrained_gradients(word_position, RNNClassifier, \n",
        "                             (vocab, embeddings, embeddings.dim, 20),\n",
        "                             {\"freeze_embeddings\": False})\n",
        "    print(\"-----------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6409a545-934d-41cd-8e94-abfc2ed4b490",
      "metadata": {
        "id": "6409a545-934d-41cd-8e94-abfc2ed4b490"
      },
      "source": [
        "**QUESTION 1.2.9 [2 points]**  \n",
        "What do you notice about how the gradients change between versions, both in terms of the size of the gradients and the correspondences between the positive and negative gradients for each version? Why do you think that is?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5bceb5-5ff5-409c-b926-89508515094f",
      "metadata": {
        "id": "5e5bceb5-5ff5-409c-b926-89508515094f"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a227eb7-ece3-4e63-b216-d401e776ec48",
      "metadata": {
        "id": "7a227eb7-ece3-4e63-b216-d401e776ec48"
      },
      "source": [
        "---\n",
        "It's clear from the above exploration that inserting the new tokens at the beginning and end results in drastically different gradients. The gradients determine how quickly the model can learn the meanings of the new tokens. In turn, this affects how quickly the model can improve its performance; each sequence always contains one of these tokens that identifies its sentiment, so the faster the model can learn their meaning, the more it can leverage that to make sure that it interprets the sentiment of the sequence correctly.\n",
        "\n",
        "However, learning about these tokens is not guaranteed to drastically improve performance! The model also has to be able to *retain* the information that it gets when it sees those tokens, until the point where it does the classification task. This is well demonstrated by the version of the model where the new tokens are always placed in the middle of the sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c92fc9-6499-4fa2-b87a-4bc117f01820",
      "metadata": {
        "id": "d7c92fc9-6499-4fa2-b87a-4bc117f01820"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "rnn_classifier_newmiddle = RNNClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False)\n",
        "optimizer = optim.SGD(rnn_classifier_newmiddle.parameters(), lr=0.01)\n",
        "train(rnn_classifier_newmiddle, train_dataloader_newmiddle, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newmiddle)\n",
        "\n",
        "newword_similarities(rnn_classifier_newmiddle)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a038db-0278-4ac2-af15-51a808fa0f6f",
      "metadata": {
        "id": "d6a038db-0278-4ac2-af15-51a808fa0f6f"
      },
      "source": [
        "**QUESTION 1.2.10. [2 points]**  \n",
        "Based on the gradients you saw earlier, would you expect the embeddings for the new tokens in this model to be better (more meaningful) or worse (less meaningful) than in the model where the new tokens are always placed at the start of the sequence? The cosine-similarity report for this model is a little mixed: identify one way in which it suggests that the embeddings are better, and one way in which it suggests that the embeddings are worse. Why do you think it is so mixed?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504c4655-b43e-40f9-b709-fde6a64c7c0e",
      "metadata": {
        "id": "504c4655-b43e-40f9-b709-fde6a64c7c0e"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29aa2f6-c719-44f2-bdcb-ad56a1651137",
      "metadata": {
        "id": "f29aa2f6-c719-44f2-bdcb-ad56a1651137"
      },
      "source": [
        "**QUESTION 1.2.11. [2 points]**  \n",
        "Let's assume that the embeddings for the new tokens are, on balance, better in this model than in the model where the new tokens are added to the start of a sequence. Under this assumption, we would expect the performance of this model (in terms of training and validation accuracies and losses) to be better as well. Is this the case? Suggest a reason why or why not (other than the assumption not being justified)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a093216-9b97-4dca-8a0e-0b0710ac3585",
      "metadata": {
        "id": "4a093216-9b97-4dca-8a0e-0b0710ac3585"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365fc0e4-5238-4d79-a47c-ce1dc3ab30ae",
      "metadata": {
        "id": "365fc0e4-5238-4d79-a47c-ce1dc3ab30ae"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556c50a8-a566-466e-8bd2-ca087745289b",
      "metadata": {
        "tags": [],
        "id": "556c50a8-a566-466e-8bd2-ca087745289b"
      },
      "source": [
        "## Activity 1.3: Improving the RNN model\n",
        "\n",
        "In Activity 1.2, we saw two main issues that can limit performance of RNNs: *gradients* and *retention*. In this activity, we'll explore some changes that improve the RNN model by addressing these issues.\n",
        "\n",
        "We'll start with a change that addresses retention without directly affecting gradients: changing the size of the recurrent (hidden) layer.\n",
        "\n",
        "One of the big issues for retention is the fact that all of the information the network keeps track of in its recurrent (hidden) layer has to be compressed to a relatively small number of units. In the models we have been using so far, we've had 50-unit word embeddings and 50-unit recurrent layers; so the model has been forced to compress a whole document down to the size of a single word! While there are certainly some redundancies that make this kind of compression reasonable - for example, the fact that words have many aspects of meaning expressed in their embeddings, and only a few of these may be relevant for the classification task at hand - it can still be beneficial to take a little pressure off by making the recurrent layer larger than the embedding layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f0bb37-669f-41e8-9e86-da692ef71855",
      "metadata": {
        "id": "07f0bb37-669f-41e8-9e86-da692ef71855"
      },
      "source": [
        "---\n",
        "---\n",
        "### Questions 1.3.1 - 1.3.2\n",
        "\n",
        "We saw an issue of retention with the model where the new tokens were inserted in the middle of the sequence. The code cell below trains a new version of this model, with 100 units in the recurrent layer rather than 50. Run the code cell to train the model and present the cosine-similarity report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6e872c-5549-4e90-99e5-f5a00376e530",
      "metadata": {
        "id": "9f6e872c-5549-4e90-99e5-f5a00376e530"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "rnn_classifier_newmiddle_bigger = RNNClassifier(vocab, embeddings, 100, 20, freeze_embeddings=False)\n",
        "optimizer = optim.SGD(rnn_classifier_newmiddle_bigger.parameters(), lr=0.01)\n",
        "train(rnn_classifier_newmiddle_bigger, train_dataloader_newmiddle, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newmiddle)\n",
        "\n",
        "newword_similarities(rnn_classifier_newmiddle_bigger)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c75067-c97c-4d47-b91f-dc7741930e0a",
      "metadata": {
        "id": "e2c75067-c97c-4d47-b91f-dc7741930e0a"
      },
      "source": [
        "**QUESTION 1.3.1. [2 points]**  \n",
        "Compared to the corresponding model you trained for Question 1.2.10, how are the training and validation accuracies and losses and the training time different here? Why does increasing the size of the recurrent layer have these effects?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06cd8bac-5e65-448b-bbfb-dc84d706c746",
      "metadata": {
        "id": "06cd8bac-5e65-448b-bbfb-dc84d706c746"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e7ccd5-ed8b-4bce-934a-bf20fd155f9f",
      "metadata": {
        "id": "e1e7ccd5-ed8b-4bce-934a-bf20fd155f9f"
      },
      "source": [
        "**QUESTION 1.3.2. [1 point]**  \n",
        "Does it seem like the quality of the embeddings the model has learned for the new tokens has changed in any way? Why do you think this is, if the gradients haven't changed?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e00c5ff9-5e11-4ab6-9c29-ffa2e7464edd",
      "metadata": {
        "id": "e00c5ff9-5e11-4ab6-9c29-ffa2e7464edd"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e814af5-4e89-4baf-9a3d-a7edc47852ad",
      "metadata": {
        "id": "0e814af5-4e89-4baf-9a3d-a7edc47852ad"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "Next, we'll consider a change that addresses how the gradients are used in training, without directly affecting retention: changing the optimizer.\n",
        "\n",
        "Up to now, we have been using the `SGD` optimizer, which nudges the weights by subtracting an amount proportional to the gradient, where the proportionality scaling factor is given by the *learning rate*. In `SGD`, the learning rate is the same for all weights across all epochs, and the nudge is solely determined by the learning rate and the current gradient. For RNNs, it is practically always better to use the `Adam` optimizer, which allows the learning rate to *decay* over epochs and to be *different* for weights that are nudged often vs. weights that are nudged rarely. It also introduces *momentum*, which lets the weights continue to be nudged according to a \"memory\" of what the gradient used to be like (rather than just the current gradient). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c86b5cd0-4779-4848-9af4-c5ba5f70a878",
      "metadata": {
        "id": "c86b5cd0-4779-4848-9af4-c5ba5f70a878"
      },
      "source": [
        "---\n",
        "---\n",
        "### Question 1.3.3\n",
        "\n",
        "Using the `Adam` optimizer in PyTorch is easy: just swap `optim.SGD()` for `optim.Adam`, and remove the `lr` keyword argument (since Adam figures out the learning rate itself). The code cell below illustrates, for the model where the new tokens are added at the start of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca719bd1-8544-404e-8ed8-2a3c17c09948",
      "metadata": {
        "id": "ca719bd1-8544-404e-8ed8-2a3c17c09948"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "rnn_classifier_newstart_adam = RNNClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False)\n",
        "optimizer = optim.Adam(rnn_classifier_newstart_adam.parameters())\n",
        "train(rnn_classifier_newstart_adam, train_dataloader_newstart, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newstart)\n",
        "\n",
        "newword_similarities(rnn_classifier_newstart_adam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7454f3dc-1b6e-417d-be08-db1ad904ca13",
      "metadata": {
        "id": "7454f3dc-1b6e-417d-be08-db1ad904ca13"
      },
      "source": [
        "**QUESTION 1.3.3. [2 points]**  \n",
        "Compared to the corresponding model you trained for Question 1.2.3, how are the training and validation accuracies and losses different here? What about the cosine-similarity report on the quality of the embeddings learned for the new tokens (comared to Question 1.2.4). What does this mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54572e97-f233-4e69-9664-4577bd1f3d9c",
      "metadata": {
        "id": "54572e97-f233-4e69-9664-4577bd1f3d9c"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890cba72-a9ac-4e1a-9500-ca55be30620e",
      "metadata": {
        "id": "890cba72-a9ac-4e1a-9500-ca55be30620e"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "Now let's consider a way of changing both retention and gradients: making the RNN bidirectional. In a bidirectional RNN, the model is trained both forward and backward in time on each sequence, and the document embedding is obtained by concatenating the final forward recurrent layer and the final backward recurrent layer. This means that:  \n",
        "\n",
        "1. the final document embedding is twice as large (like in Questions 1.3.1 - 1.3.2); and  \n",
        "2. tokens at both the start and the end of the sequence are encountered \"right before\" the final document embedding is created.\n",
        "\n",
        "PyTorch makes training a bidirectional RNN easy, through a Boolean argument `bidirectional` in `nn.RNN()`; in our `RNNClassifier()` class, this is provided through the `recurrent_bidirectional` keyword argument. The `forward()` method also needs to be updated to reflect the fact that the document embedding is twice the size of `recurrent_dim` (our `RNNClassifier()` class takes care of this automatically)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c7d4b3-727e-4a6a-814c-65922ade4992",
      "metadata": {
        "id": "98c7d4b3-727e-4a6a-814c-65922ade4992"
      },
      "source": [
        "---\n",
        "---\n",
        "### Questions 1.3.4 - 1.3.6\n",
        "\n",
        "Run the code cell below to train a bidirectional RNN on the data where new tokens have been added at the start of each sequence. (This model still uses the `SGD` optimizer, for comparability with earlier models; when `Adam` is used instead, it hits a performance ceiling within 3 epochs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a34e02-81c2-4dbb-9009-9acb17f95808",
      "metadata": {
        "id": "d6a34e02-81c2-4dbb-9009-9acb17f95808"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "birnn_classifier_newstart = RNNClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False, recurrent_bidirectional=True)\n",
        "optimizer = optim.SGD(birnn_classifier_newstart.parameters(), lr=0.01)\n",
        "train(birnn_classifier_newstart, train_dataloader_newstart, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newstart)\n",
        "\n",
        "newword_similarities(birnn_classifier_newstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0db2304-7774-4050-bc39-315aaf9a8e10",
      "metadata": {
        "id": "e0db2304-7774-4050-bc39-315aaf9a8e10"
      },
      "source": [
        "**QUESTION 1.3.4. [2 points]**  \n",
        "What do you notice about the training performance, training time, and cosine-similarity report, as compared to the corresponding unidirectional model you trained for Question 1.2.3? Why do these differences exist?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01c29c0-56b5-44b7-8fd8-c7de9cd2071c",
      "metadata": {
        "id": "c01c29c0-56b5-44b7-8fd8-c7de9cd2071c"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb260ab-e690-4aa8-99a4-7bac666c4bf0",
      "metadata": {
        "id": "edb260ab-e690-4aa8-99a4-7bac666c4bf0"
      },
      "source": [
        "The code cell below prints the gradients for the bidirectional RNN based on the position at which the new tokens are inserted. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd3621e-922b-468f-89de-1b2185ba416e",
      "metadata": {
        "id": "2fd3621e-922b-468f-89de-1b2185ba416e"
      },
      "outputs": [],
      "source": [
        "for word_position in [\"start\", \"middle\", \"end\"]:\n",
        "    view_untrained_gradients(word_position, RNNClassifier, \n",
        "                             (vocab, embeddings, embeddings.dim, 20),\n",
        "                             {\"freeze_embeddings\": False, \"recurrent_bidirectional\": True})\n",
        "    print(\"-----------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dfe4403-b861-48e1-9bd6-256ea2fc58f1",
      "metadata": {
        "id": "1dfe4403-b861-48e1-9bd6-256ea2fc58f1"
      },
      "source": [
        "**QUESTION 1.3.5. [2 points]**  \n",
        "In broad strokes, what is the same as you saw for the gradients before (for Question 1.2.9), and what is different? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10b31d2-13aa-456d-ab20-af733bd00399",
      "metadata": {
        "id": "c10b31d2-13aa-456d-ab20-af733bd00399"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6966b3-715e-4516-8532-8527919437ee",
      "metadata": {
        "id": "3c6966b3-715e-4516-8532-8527919437ee"
      },
      "source": [
        "The code cell below trains a bidirectional RNN on the data where the new tokens are inserted in the middle of a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce96c4d-5430-48c1-9923-42627ee9ee36",
      "metadata": {
        "id": "3ce96c4d-5430-48c1-9923-42627ee9ee36"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "birnn_classifier_newmiddle = RNNClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False, recurrent_bidirectional=True)\n",
        "optimizer = optim.SGD(birnn_classifier_newmiddle.parameters(), lr=0.01)\n",
        "train(birnn_classifier_newmiddle, train_dataloader_newmiddle, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newmiddle)\n",
        "\n",
        "newword_similarities(birnn_classifier_newmiddle)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38c5293-2c24-4b14-9931-e064c65e9bd5",
      "metadata": {
        "id": "a38c5293-2c24-4b14-9931-e064c65e9bd5"
      },
      "source": [
        "**QUESTION 1.3.6. [1 point]**  \n",
        "Why is this model not as good as the one where the new tokens are inserted at the start of the sequence?  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44f28581-0e7b-474a-9304-ee2cfe121df0",
      "metadata": {
        "id": "44f28581-0e7b-474a-9304-ee2cfe121df0"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fbf0c95-986b-4282-950f-378f1d538d65",
      "metadata": {
        "id": "7fbf0c95-986b-4282-950f-378f1d538d65"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "To help the model deal with the case where the new tokens are inserted in the middle of the sequence, we need to make one final change that affects both retention and gradients: changing from a simple RNN architecture to an LSTM architecture. \n",
        "\n",
        "Again, PyTorch makes this easy: just substitute the `nn.RNN()` layer in the model for a lookalike `nn.LSTM()` layer. The code cell below creates a `LSTMClassifer` class that is a factory for making LSTM models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3be949-6b81-4703-bfb9-6903d7f48362",
      "metadata": {
        "id": "5a3be949-6b81-4703-bfb9-6903d7f48362"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab, embeddings, recurrent_dim, hidden_dim, freeze_embeddings=True,\n",
        "                 recurrent_layers=1, recurrent_bidirectional=False):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        \n",
        "        vocab_embeddings = embeddings.get_vecs_by_tokens(self.vocab.get_itos())\n",
        "        padding_idx = self.vocab.get_stoi().get(\"<pad>\")  # Get the <pad> index\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab_embeddings, freeze=freeze_embeddings, \n",
        "                                                      padding_idx=padding_idx) # Tell PyTorch that <pad> is for padding\n",
        "        \n",
        "        # The embeddings go into an RNN layer with recurrent_dim units\n",
        "        self.recurrent_layer = nn.LSTM(embeddings.dim, recurrent_dim,\n",
        "                                       num_layers=recurrent_layers, bidirectional=recurrent_bidirectional,\n",
        "                                       batch_first=True) # Because we'll make the mini-batch a list of sequences\n",
        "        \n",
        "        # The recurrent output creates a doc_embedding, which feeds into a of hidden_dim units\n",
        "        # We'll be concatenating the forward and backward direction of all layers\n",
        "        # from the recurrent output, so the doc_embedding will be sized accordingly\n",
        "        doc_embedding_dim = recurrent_dim * recurrent_layers * int(1 + recurrent_bidirectional)\n",
        "        self.hidden_layer = nn.Sequential(\n",
        "            nn.Linear(doc_embedding_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # The output layer will go from the hidden layer (hidden_dim units) to a single unit\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.loss_function = nn.BCELoss()\n",
        "\n",
        "    def forward(self, padded_text, seq_lengths):\n",
        "        word_embeddings = self.embedding(padded_text)\n",
        "        \n",
        "        # The sequence of word embeddings has to be packed for efficiency of the RNN\n",
        "        packed_word_embeddings = pack_padded_sequence(word_embeddings, seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "        (final_layer_all_timesteps, (all_layers_final_timestep, _)) = self.recurrent_layer(packed_word_embeddings)\n",
        "        \n",
        "        # all_layers_final_timestep contains the activations of all (stacked / bidirectional) recurrent\n",
        "        # layers at the final timestep for each sequence (taking the padding into account).\n",
        "        # For our classifier, we will stick all of these layers together (forward + backward, \n",
        "        # for each stacked layer) to use as the document embedding.\n",
        "        # all_layers_final_timestep has shape (num_layers, minibatch_size, recurrent_dim);\n",
        "        # we want something of shape (minibatch_size, num_layers * recurrent_dim),\n",
        "        # so we reorder the dimensions and then reshape to stick everything together\n",
        "        minibatch_size = all_layers_final_timestep.size(1)\n",
        "        doc_embedding = all_layers_final_timestep.permute(1, 0, 2).reshape(minibatch_size, -1)\n",
        "        \n",
        "        hidden = self.hidden_layer(doc_embedding)\n",
        "        output = self.output_layer(hidden)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24836213-1643-4c74-aec2-9b054300fab3",
      "metadata": {
        "id": "24836213-1643-4c74-aec2-9b054300fab3"
      },
      "source": [
        "---\n",
        "---\n",
        "### Questions 1.3.7 - 1.3.8\n",
        "\n",
        "Let's train a unidirectional LSTM on the data where new tokens have been added at the start of the sequence (the hardest case!), using the `Adam` optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cc3fc2-1225-4233-92d0-4aa9ffbab147",
      "metadata": {
        "id": "b1cc3fc2-1225-4233-92d0-4aa9ffbab147"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "lstm_classifier_newstart = LSTMClassifier(vocab, embeddings, embeddings.dim, 20, freeze_embeddings=False)\n",
        "optimizer = optim.Adam(lstm_classifier_newstart.parameters())\n",
        "train(lstm_classifier_newstart, train_dataloader_newstart, optimizer, epochs=10, print_every=1, validation_data=test_dataloader_newstart)\n",
        "\n",
        "newword_similarities(lstm_classifier_newstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1c6c99-67bf-491e-b8f7-69d0a821c58c",
      "metadata": {
        "id": "dc1c6c99-67bf-491e-b8f7-69d0a821c58c"
      },
      "source": [
        "**QUESTION 1.3.7. [2 points]**  \n",
        "Compare this model to the corresponding RNN you trained for question 1.3.3. What does this model seem to have done better? Intuitively, what is it about the LSTM architecture that you think has let it make those improvements?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed1937c3-8526-4eb0-9525-0e95612db21f",
      "metadata": {
        "id": "ed1937c3-8526-4eb0-9525-0e95612db21f"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697f579c-b0f6-4072-baba-1294110479bb",
      "metadata": {
        "id": "697f579c-b0f6-4072-baba-1294110479bb"
      },
      "source": [
        "**QUESTION 1.3.8. [2 points]**  \n",
        "Is there anything that this model seems to do worse, or that we might want to be wary of when using LSTMs in general? Intuitively, what is it about the LSTM architecture that is behind these (potential) issues?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ac10e4-062f-4736-8583-4d467449e06b",
      "metadata": {
        "id": "64ac10e4-062f-4736-8583-4d467449e06b"
      },
      "source": [
        "*Double-click on this cell to enter your response.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc1aaac-dd53-496b-8ada-075f4b5f29a7",
      "metadata": {
        "id": "9fc1aaac-dd53-496b-8ada-075f4b5f29a7"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f4daf7-216e-43a8-9a2f-aace7f1e52a7",
      "metadata": {
        "id": "84f4daf7-216e-43a8-9a2f-aace7f1e52a7"
      },
      "source": [
        "# Part 2: Simple RNNs from scratch [50 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd99e20a-f503-4514-a64e-09004b5aac9d",
      "metadata": {
        "cell_id": "00001-72aa6fcd-ab1c-45dd-98a5-90cf247739d7",
        "deepnote_cell_type": "markdown",
        "id": "dd99e20a-f503-4514-a64e-09004b5aac9d"
      },
      "source": [
        "In this part of the assignment, you'll build an understanding of what's going on \"under the hood\" in simple recurrent networks, by implementing the forward and backward passes of simple RNNs yourself, and using them to train a network for a single epoch. (We won't put everything together into a class like we did in Assignment 1, but you could do so if you wanted to!)\n",
        "\n",
        "Run the code cell below first, to import the modules you'll need for this part of the assignment. The code cell also defines several functions that you wrote in Assignment 1. *(Note: the function that was previously `logistic_derivative()` is now `sigmoid_derivative()` and the function that was previously `bce_loss_gradient()` is now `crossent_loss_gradient()`. We have also added (optional) keyword arguments to `calculate_weights_gradient()` and `send_delta_back()` to account for the fact that the recurrent weights in RNNs do not have bias terms.)*\n",
        "\n",
        "*Note: this assignment will use NumPy, but you will not need to know any more about it than what you needed for Assignment 1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06ffbb4-33f5-4ad7-9ccd-a54451d4bcaa",
      "metadata": {
        "cell_id": "00003-9f3bde15-181e-44a8-88e9-5fc68598900e",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 888,
        "execution_start": 1619508614561,
        "id": "f06ffbb4-33f5-4ad7-9ccd-a54451d4bcaa",
        "output_cleared": true,
        "source_hash": "202f447a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from util import *\n",
        "    \n",
        "def insert_bias(values):\n",
        "    \"\"\"Inserts a 1 at the beginning of a vector of values.\"\"\"\n",
        "    return np.insert(values, 0, 1)\n",
        "\n",
        "def logistic_layer(values, weights):\n",
        "    \"\"\"Calculates the result of passing a set of values through a logistic\n",
        "    layer of a neural network with given weights.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    values: np.array representing a vector of size (num_features,);\n",
        "            does not contain a value for the bias term\n",
        "    weights: np.array representing a vector of size (num_features + 1,)\n",
        "             or a matrix of size (num_outputs, num_features + 1);\n",
        "             the first element (in each row) corresponds to the bias term\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    activations: if weights is a vector, this is a float;\n",
        "                 if weights is a matrix, this is a vector of size (num_outputs,)\n",
        "    \"\"\"\n",
        "    values = insert_bias(values)\n",
        "    evidence = linear(weights, values)\n",
        "    activations = sigmoid(evidence)\n",
        "    return activations\n",
        "\n",
        "def crossent_loss_gradient(predicted, actual):\n",
        "    \"\"\"Returns the gradient of the cross-entropy loss for a single training sample,\n",
        "    based on the predicted probability and the actual class for that sample\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    predicted: np.array or float; the probability(ies) output by the classifier\n",
        "               for a given training sample\n",
        "    actual: np.array or int; the actual class / probability(ies) for the training\n",
        "            sample (0 for default, 1 for alternative)\n",
        "    \"\"\"\n",
        "    return predicted - actual\n",
        "\n",
        "def calculate_weights_gradient(delta, values, insert_bias_term=True):\n",
        "    \"\"\"Combines local gradients delta with feature values in order to \n",
        "    calculate the weights gradient of a layer of a neural network.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    delta: np.array representing a vector of size (num_units,),\n",
        "           containing the local gradient of each unit with respect to its input\n",
        "    values: np.array representing a vector of size (num_features,),\n",
        "            containing the values of features that combine with weights to\n",
        "            form inputs to the units; does not contain a value for the bias term\n",
        "    insert_bias_term: bool (default True); indicates whether to insert an element to the\n",
        "                      values vector to account for the bias term (i.e., whether the weights\n",
        "                      matrix being nudged has a row for the bias term)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    weights_gradient: np.array representing a matrix of size (num_units, num_features + 1)\n",
        "                      (this is a row vector if num_units == 1), which contains the gradient\n",
        "                      of the loss function with respect to the weights of the current layer\n",
        "    \"\"\"\n",
        "    if insert_bias_term:\n",
        "        values = insert_bias(values)\n",
        "    weights_gradient = np.outer(delta, values)\n",
        "    if isinstance(weights_gradient, np.ndarray) and weights_gradient.shape[0] == 1:\n",
        "        weights_gradient = weights_gradient.flatten()\n",
        "    return weights_gradient\n",
        "\n",
        "def send_delta_back(delta, weights, remove_bias_term=True):\n",
        "    \"\"\"Obtains a signal vector by sending local gradients backward through\n",
        "    weights. The element of the signal vector corresponding to the bias unit\n",
        "    can be removed prior to returning, so that the output has the same shape as\n",
        "    the previous layer activation derivatives.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    delta: np.array representing a vector of size (curr_layer_size,),\n",
        "           containing the local gradients at the current layer (i)\n",
        "    weights: np.array representing a matrix of size (prev_layer_size + 1, curr_layer_size),\n",
        "             containing the weights that lead into the current layer (i)\n",
        "    remove_bias_term: bool (default True); indicates whether to remove the element of the signal\n",
        "                      vector prior to returning (i.e., whether the weights matrix has a row for\n",
        "                      the bias term)\n",
        "             \n",
        "    Returns\n",
        "    -------\n",
        "    signal: np.array representing a vector of size (prev_layer_size,),\n",
        "            containing the signal (precursor to local gradients) at the\n",
        "            previous layer (i-1). The element corresponding to the bias\n",
        "            unit has been removed.\n",
        "    \"\"\"\n",
        "    signal = np.atleast_2d(weights).T @ np.atleast_1d(delta) # make sure the shapes align\n",
        "    signal = signal.flatten() # convert back to a vector\n",
        "    if remove_bias_term:\n",
        "        trimmed_signal = np.delete(signal, 0) # remove the first element\n",
        "        return trimmed_signal\n",
        "    else:\n",
        "        return signal\n",
        "\n",
        "def linear(weights, values):\n",
        "    \"\"\"Returns the matrix algebra product of weights and values,\n",
        "    both represented as NumPy arrays.\n",
        "    \"\"\"\n",
        "    return weights @ values\n",
        "\n",
        "def sigmoid(evidence):\n",
        "    \"\"\"Applies the logistic function to convert evidence to probability.\"\"\"\n",
        "    return 1 / (1 + np.exp(-evidence))\n",
        "\n",
        "def sigmoid_derivative(logistic_output):\n",
        "    \"\"\"Returns the derivative of the logistic function, calculated from the\n",
        "    output of the logistic function at a particular point.\n",
        "    \"\"\"\n",
        "    return logistic_output * (1 - logistic_output)\n",
        "\n",
        "def tanh(z):\n",
        "    \"\"\"Applies the tanh function to calculate activation, given input z.\"\"\"\n",
        "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
        "\n",
        "def tanh_derivative(tanh_output):\n",
        "    \"\"\"Returns the derivative of the tanh function, calculated from the\n",
        "    output of the tanh function at a particular point.\n",
        "    \"\"\"\n",
        "    return 1 - (tanh_output ** 2)\n",
        "\n",
        "def relu(z):\n",
        "    \"\"\"Applies the ReLU function to calculate activation, given input z.\"\"\"\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "def relu_derivative(relu_output):\n",
        "    \"\"\"Returns the derivative of the ReLU function, calculated from the\n",
        "    output of the ReLU function at a particular point.\n",
        "    \"\"\"\n",
        "    if isinstance(relu_output, np.ndarray):\n",
        "        return (relu_output > 0).astype(int)\n",
        "    else:\n",
        "        return int(relu_output > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80120ac1-250a-47af-a0f4-fed4f8a8e40c",
      "metadata": {
        "id": "80120ac1-250a-47af-a0f4-fed4f8a8e40c"
      },
      "source": [
        "## Activity 2.1: The forward pass in a sequence classifier\n",
        "\n",
        "In this first activity, you'll implement the forward pass of a unidirectional (left-to-right) recurrent neural classifier with a single hidden layer (which uses ReLU by default, but which can be changed easily).\n",
        "\n",
        "Here is a visualization of the structure of the network:\n",
        "\n",
        "![RNN sequence classifier structure](https://github.com/ucsb-ling111/assignment2-giovani-gutierrez/blob/main/images/rnn_classifier.png?raw=1)\n",
        "\n",
        "*Note: in this activity, we will assume that the sequence embedding, which is the activation of the hidden layer at the final timestep, is used directly as the input to the output layer. This is slightly different to the example we saw in class, where there was a hidden layer between the sequence embedding and the output layer.*\n",
        "\n",
        "We'll use a slight adaptation of the notation from the textbook in this activity:  \n",
        "- $\\mathbf{x}_t$ is the vector of values that is input to the network at time step $t$, which has shape `(num_features,)`;  \n",
        "- $\\mathbf{z}_t$ is the vector of inputs to the hidden layer activation function at time step $t$, which has shape `(num_hidden,)`;  \n",
        "- $\\mathbf{h}_t$ is the vector of hidden unit activations at time step $t$, which has shape `(num_hidden,)`;  \n",
        "- $y$ is the input to the output unit after the entire sequence has been processed;  \n",
        "- $p$ is the output of the network after the entire sequence has been processed, which represents the probability of deviating from the default class for that sequence;  \n",
        "- $W$ is the matrix of *input weights* leading from the inputs of a given timestep, $\\mathbf{x}_t$, to the hidden layer of that timestep, $\\mathbf{h}_t$. This is a common matrix of shape `(num_hidden, num_features + 1)` that is used at all timesteps;  \n",
        "- $U$ is the matrix of *recurrent weights* leading from the hidden layer of the previous timestep, $\\mathbf{h}_{t-1}$, to the hidden layer of the current timestep, $\\mathbf{h}_t$. This is a common matrix of shape `(num_hidden, num_hidden)` that is used between all pairs of adjacent timesteps;  \n",
        "- $\\mathbf{v}^T$ is the vector of *output weights* leading from the hidden layer of the final timestep, $\\mathbf{h}_n$, to the output, $y$. It has shape `(num_hidden + 1,)` *(note: the textbook denotes this as a matrix rather than a vector for generality)*.\n",
        "- $g(\\cdot)$ is the activation function of the hidden layer.  \n",
        "- $\\sigma(\\cdot)$ is the logistic function.\n",
        "\n",
        "With this notation, the forward pass for a sequence with $n$ timesteps (from 1 to $n$) can be represented as follows:\n",
        "\n",
        "#### RNN CLASSIFIER CALCULATION STEPS\n",
        "\n",
        "1. calculate the input to the hidden layer activation function at timestep 1, based on the input values at timestep 1: $\\mathbf{z}_1$ = $W\\mathbf{x}_1$\n",
        "2. calculate the hidden layer activations at timestep 1: $\\mathbf{h}_1$ = $g(\\mathbf{z}_1)$\n",
        "3. for each timestep $t$ from 2 to $n$:  \n",
        "   1. calculate the input to the hidden layer activation function at that timestep, based on the input values at that timestep and the hidden layer activations at the previous timestep: $\\mathbf{z}_t$ = $W\\mathbf{x}_t + U\\mathbf{h}_{t-1}$\n",
        "   2. calculate the hidden layer activations at that timestep: $\\mathbf{h}_t$ = $g(\\mathbf{z}_t)$\n",
        "4. calculate the input to the output activation function for the network, based on the hidden layer activations at the final timestep: $y$ = $\\mathbf{v}^T \\mathbf{h}_n$\n",
        "5. calculate the network's output activation: $p$ = $\\sigma(y)$\n",
        "\n",
        "*Note: the shapes in the definitions above do not quite match up because of the bias terms, which are built into $W$ and $\\mathbf{v}^T$ but not $\\mathbf{x}_t$ and $\\mathbf{h}_t$. We will explain how the bias terms work as we progress through the activity.*\n",
        "\n",
        "Most of these steps are exactly the same as in a feedforward neural network! There are three big differences:  \n",
        "- in step 3A, the hidden layer at timestep $t$ has *two* inputs, one from the sequence value at that timestep ($\\mathbf{x}_t$), and one from the hidden layer at the previous timestep ($\\mathbf{h}_{t-1}$);  \n",
        "- the for loop in step 3 means the calculation steps could be performed any number of times, depending on the length of the input sequence;  \n",
        "- throughout, the *same* input and recurrent weights ($W$ and $U$) are used repeatedly for calculation steps at different timepoints, rather than having different weights for each step of the calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8e32a6-7bbc-466f-9d55-c48e506a05ea",
      "metadata": {
        "id": "fb8e32a6-7bbc-466f-9d55-c48e506a05ea"
      },
      "source": [
        "### Task 2.1.1: Calculating inputs to hidden layer activation functions [2 points]\n",
        "\n",
        "We have provided a function `linear()` which multiplies a vector of values by a matrix of weights. In a feedforward neural network, we could use this function to calculate the input to the activation function of any layer, as follows:  \n",
        "1. augment the values vector by inserting a value for the bias term, using the `insert_bias()` function;  \n",
        "2. multiply the augmented values vector by the weights, using the `linear()` function.\n",
        "\n",
        "For the hidden layer in a recurrent neural network, this works for the first timestep ([RNN classifier calculation step 1](#RNN-CLASSIFIER-CALCULATION-STEPS)), but not for other timesteps ([RNN classifier calculation step 3A](#RNN-CLASSIFIER-CALCULATION-STEPS)). To make it work in general, we have to bring in the hidden layer activations from the previous timestep (if they exist):  \n",
        "1. augment the values vector by inserting a value for the bias term, using the `insert_bias()` function;  \n",
        "2. multiply the augmented values vector by the input weights $W$, using the `linear()` function;  \n",
        "3. multiply the vector of hidden layer activations from the previous timestep by the recurrent weights $U$, using the `linear()` function;  \n",
        "4. add together the vectors resulting from (2) and (3).\n",
        "\n",
        "Note that **only the values vector is augmented for the bias term**; the hidden layer activations from the previous timestep (in step 3) are directly multiplied by the recurrent weights, without first having to be augmented for a bias term. This is because we only need to account for the bias **once**, which we do by incorporating it into the input weights $W$ as in feedforward neural networks.\n",
        "\n",
        "Write a function `recurrent_linear()` which adapts the `linear()` function for recurrent neural networks. Your function should take as arguments:  \n",
        "- a vector of `current_values` (of shape `(num_features,)`); and  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element corresponds to a bias term and the subsequent elements correspond to the elements of `values`);  \n",
        "\n",
        "as well as **optional keyword arguments** for:  \n",
        "- a vector of `prev_hidden` activations (of shape `(num_hidden,)`); and  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`).\n",
        "\n",
        "It should return the input to the hidden layer activation function, `current_z` ($\\mathbf{z}_t$). \n",
        "\n",
        "If the optional keyword arguments are provided, your function should carry out steps 1-4 of the calculation above; if they are not provided (i.e., if `prev_hidden` is `None`), it should only carry out steps 1-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00400e75-0275-499e-911f-90f7f1b28212",
      "metadata": {
        "id": "00400e75-0275-499e-911f-90f7f1b28212"
      },
      "outputs": [],
      "source": [
        "def recurrent_linear(current_values, input_weights,\n",
        "                    prev_hidden=None, recurrent_weights=None):\n",
        "    \"\"\"Returns the input to the hidden layer activation function at the current\n",
        "    timestep, based on the input values to the network at the current timestep\n",
        "    and the hidden layer activations from the previous timestep (if they exist).\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    current_values: np.array representing a vector of size (num_features,);\n",
        "                    represents the input to the network at the current timestep.\n",
        "                    Does not contain a value for the bias term.\n",
        "    input_weights: np.array representing a matrix of size (num_hidden, num_features + 1);\n",
        "                   represents the weights between the network input and hidden layer.\n",
        "                   The first element in each row corresponds to the bias term.\n",
        "    prev_hidden: np.array representing a vector of size (num_hidden,);\n",
        "                 represents the hidden layer activations from the previous timestep.\n",
        "                 Does not contain a value for the bias term. \n",
        "                 If None, not included in calculations.\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between previous and current hidden layers.\n",
        "                       Does not contain values for bias terms.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    current_z: a vector of size (num_hidden,)\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f2941f-94fb-4a19-b5bb-12eaa4b6927c",
      "metadata": {
        "id": "83f2941f-94fb-4a19-b5bb-12eaa4b6927c"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf6cc89-e056-4488-a7df-38abae5167c2",
      "metadata": {
        "id": "9cf6cc89-e056-4488-a7df-38abae5167c2"
      },
      "outputs": [],
      "source": [
        "check(recurrent_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a4739b-e90e-452d-a2c9-7d5226a12ac0",
      "metadata": {
        "id": "35a4739b-e90e-452d-a2c9-7d5226a12ac0"
      },
      "source": [
        "### Task 2.1.2: Getting hidden layer activations at a new timestep [2 points]\n",
        "\n",
        "At each timestep of a recurrent neural network, there are hidden layer activations to calculate ([RNN classifier calculation steps 3A-3B](#RNN-CLASSIFIER-CALCULATION-STEPS)). This involves calculating the input to the hidden layer activation function at that timestep (using the `recurrent_linear()` function) and then passing it through the activation function, which in this case is ReLU (using the `relu()` function).\n",
        "\n",
        "Write a function `get_hidden()` that calculates the hidden layer activations at a new timestep. Your function should take as arguments:  \n",
        "- a vector of `prev_hidden` activations (of shape `(num_hidden,)`;  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a vector of `input_values` (of shape `(num_features,)`);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element corresponds to a bias term); and  \n",
        "- an `activation_function` (which is `relu` by default). \n",
        "\n",
        "It should return a vector `hidden` (of shape `(num_hidden,)`) which contains the hidden layer activations at the current timestep.\n",
        "\n",
        "*Note: as demonstrated in this example, functions can be passed as arguments to other functions! Pass the name of the function as argument, without parentheses after it that would cause it to be evaluated; then it can be used with the corresponding argument name. In this example, passing `activation_function=relu` as an argument means that you can use `activation_function(z)` inside the definition of `get_hidden()` and get back the same thing as if you had written `relu(z)` instead. Because the activation function is an argument, you can reuse `get_hidden()` exactly as written with different activation functions, e.g. `activation_function=tanh` or `activation_function=sigmoid`, without having to recreate different versions of `get_hidden()` that are specific for each activation function.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8fbb6a-6a63-4847-9bad-ceca1ebe9283",
      "metadata": {
        "id": "3a8fbb6a-6a63-4847-9bad-ceca1ebe9283"
      },
      "outputs": [],
      "source": [
        "def get_hidden(prev_hidden, recurrent_weights, input_values, input_weights,\n",
        "               activation_function=relu):\n",
        "    \"\"\"Gets the hidden layer activations at the current timestep.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    prev_hidden: np.array representing a vector of size (num_hidden,);\n",
        "                 represents the hidden layer activations from the previous timestep.\n",
        "                 Does not contain a value for the bias term. \n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    input_values: np.array representing a vector of size (num_features,);\n",
        "                  represents the inputs to the network at the current timestep.\n",
        "                  Does not contain a value for the bias term.\n",
        "    input_weights: np.array representing a matrix of size (num_hidden, num_features + 1);\n",
        "                   represents the weights between the network input and hidden layer.\n",
        "                   The first element in each row corresponds to the bias term.\n",
        "    activation_function: function; the activation function for the hidden layer\n",
        "                         (default is ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    hidden: a vector of hidden layer activations at the current timestep, of shape (num_hidden,)\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "161afed8-284a-4aa7-b39b-16d38f510b1b",
      "metadata": {
        "id": "161afed8-284a-4aa7-b39b-16d38f510b1b"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3abf4c7-1de2-4b1f-90e8-77da6661ecf8",
      "metadata": {
        "id": "b3abf4c7-1de2-4b1f-90e8-77da6661ecf8"
      },
      "outputs": [],
      "source": [
        "check(get_hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f06c13c2-a0af-49c4-89bd-88f552c3397b",
      "metadata": {
        "id": "f06c13c2-a0af-49c4-89bd-88f552c3397b"
      },
      "source": [
        "### Task 2.1.3: Getting the activations at all hidden layers [3 points]\n",
        "\n",
        "The input to a recurrent neural network is not just one values vector, but a whole *sequence* of values vectors, one for each timepoint. There is a corresponding sequence of hidden layer activations. In order to make predictions with the network, we only need access to the hidden layer activations from the *final* timepoint (after processing the entire sequence); however, in order to train the network, we will need access to the hidden layer activations from *each* timepoint.\n",
        "\n",
        "Write a function `get_hidden_sequence()` that processes a sequence of input vectors through a recurrent neural network and returns a corresponding sequence of hidden layer activation vectors. Your function should take as arguments:  \n",
        "- a list of values vectors `values_list` (where the entries in the list are vectors of shape `(num_features,)`, arranged in order from timestep 1 to timestep $n$);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`); and  \n",
        "- an `activation_function` (which is `relu` by default). \n",
        "\n",
        "It should return a list of hidden layer activation vectors `hidden_list`, containing $n$ vectors each of shape `(num_hidden,)`, arranged in a corresponding order to `values_list`.\n",
        "\n",
        "*Note: your function will need to perform everything contained within [RNN classifier calculation steps 1-3](#RNN-CLASSIFIER-CALCULATION-STEPS). You should be able to do so just by putting together the functions you have already written.*\n",
        "\n",
        "*Hint: don't forget to pass the `activation_function` argument to `update_hidden()`, as in `update_hidden(..., activation_function=activation_function)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbcfd43-6a25-41ec-b488-10bb5e6db78c",
      "metadata": {
        "id": "ffbcfd43-6a25-41ec-b488-10bb5e6db78c"
      },
      "outputs": [],
      "source": [
        "def get_hidden_sequence(values_list, input_weights, recurrent_weights, \n",
        "                        activation_function=relu):\n",
        "    \"\"\"Processes a list of input values vectors and returns a list of hidden layer\n",
        "    activations at all timepoints.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    values_list: list of np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector\n",
        "                 of input values for a particular timestep.\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    input_weights: np.array representing a matrix of size (num_hidden, num_features + 1);\n",
        "                   represents the weights between the network input and hidden layer.\n",
        "                   The first element in each row corresponds to the bias term.\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    activation_function: function; the activation function for the hidden layer\n",
        "                         (default is ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    hidden_list: a list of vectors of hidden layer activations at each timestep, \n",
        "                 each of shape (num_hidden,)\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c7cb67-2e71-4866-90c8-6778f7a9bc83",
      "metadata": {
        "id": "b0c7cb67-2e71-4866-90c8-6778f7a9bc83"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0dd1de3-2b3c-4b07-9782-cc1cf404ca2c",
      "metadata": {
        "id": "a0dd1de3-2b3c-4b07-9782-cc1cf404ca2c"
      },
      "outputs": [],
      "source": [
        "check(get_hidden_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8499d4-f3e6-4a8c-a967-6db61b313a2f",
      "metadata": {
        "id": "5f8499d4-f3e6-4a8c-a967-6db61b313a2f"
      },
      "source": [
        "### Task 2.1.4: The complete forward pass [2 points]\n",
        "\n",
        "In the forward pass, the network calculates the hidden layer activations at each timestep ([RNN classifier calculation steps 1-3](#RNN-CLASSIFIER-CALCULATION-STEPS)), and then makes a prediction based on the hidden layer activations at the final timestep ([RNN classifier calculation steps 4-5](#RNN-CLASSIFIER-CALCULATION-STEPS)). This prediction is made using a `logistic_layer()`, exactly like logistic regression or a feedforward neural network.\n",
        "\n",
        "In order to be able to train the network efficiently, the forward pass should return not only the final prediction probability, but also the hidden layer activations at each timestep. These will be used in backprop, just like we saw in Assignment 1 for feedfoward neural networks.\n",
        "\n",
        "Write a function `recurrent_classifier_forward()` that calculates the output and intermediate hidden layer activations of a 2-layer recurrent neural network, as described in the [RNN classifier calculation steps](#RNN-CLASSIFIER-CALCULATION-STEPS). Your function should take as arguments:  \n",
        "- a list of values vectors `values_list` (where the entries in the list are vectors of shape `(num_features,)`, arranged in order from timestep 1 to timestep $n$);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first element corresponds to a bias term); and  \n",
        "- an `activation_function` (which is `relu` by default). \n",
        "\n",
        "It should return a **tuple** with two elements, where:  \n",
        "- the first element is the list of hidden layer activations for each timestep; and  \n",
        "- the second element is the ultimate output of the network (i.e., the classification probability for the sequence).\n",
        "\n",
        "*Hint: the addition of a bias term for the output prediction is already handled by the `logistic_layer()` function.*\n",
        "\n",
        "*Hint: don't forget to pass the `activation_function` argument to `get_hidden_sequence()`, as in `get_hidden_sequence(..., activation_function=activation_function)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04cc329f-39dc-427b-8b2b-93c7cd27ed40",
      "metadata": {
        "tags": [],
        "id": "04cc329f-39dc-427b-8b2b-93c7cd27ed40"
      },
      "outputs": [],
      "source": [
        "def recurrent_classifier_forward(values_list, input_weights, recurrent_weights, output_weights, \n",
        "                                 activation_function=relu):\n",
        "    \"\"\"Sends an input sequence forward through a 2-layer recurrent neural network\n",
        "    that aims to classify the entire sequence.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    values_list: list of np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector\n",
        "                 of input values for a particular timestep.\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    input_weights: np.array representing a matrix of size (num_hidden, num_features + 1);\n",
        "                   represents the weights between the network input and hidden layer.\n",
        "                   The first element in each row corresponds to the bias term.\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    output_weights: np.array representing a vector of shape (num_hidden + 1,);\n",
        "                    represents the weights between the final hidden layer and the output unit.\n",
        "                    The first element corresponds to the bias term.\n",
        "    activation_function: function; the activation function for the hidden layer\n",
        "                         (default is ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a pair (hidden_list, output_prob), where\n",
        "            hidden_list: a list of vectors of hidden layer activations at each timestep, \n",
        "                         each of shape (num_hidden,)\n",
        "            output_prob: float representing the output (probability) value of the network\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4411efe2-b63b-4ee9-af8b-8313585b38d7",
      "metadata": {
        "id": "4411efe2-b63b-4ee9-af8b-8313585b38d7"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0721f036-2bca-42e0-bf92-bab76fa18e8f",
      "metadata": {
        "id": "0721f036-2bca-42e0-bf92-bab76fa18e8f"
      },
      "outputs": [],
      "source": [
        "check(recurrent_classifier_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad71693-73ee-4b62-961d-2f35c3c36c19",
      "metadata": {
        "tags": [],
        "id": "bad71693-73ee-4b62-961d-2f35c3c36c19"
      },
      "source": [
        "## Activity 2.2: The backward pass in a sequence classifier\n",
        "\n",
        "To train our recurrent neural network, we will need to implement a backward pass that calculates all the weights gradients, which are used to nudge the weights. Like in feedforward neural networks, we can do this by *back-propagating* the error of the classification (as a *local gradient*). \n",
        "\n",
        "The basic rules of back-propagation are the same as they are in feedfoward neural networks:  \n",
        "- when we send local gradients backward, we multiply them by the weights they pass through and the activation derivatives of the units they enter;  \n",
        "- when we calculate the gradients of weights between two layers, we do so by taking the outer product between local gradients at one layer and outputs of the previous layer.\n",
        "\n",
        "But there are three major differences:  \n",
        "- we have to send the errors back through *timesteps* of the recurrent hidden layer, and not just back from the output layer to the hidden layer;  \n",
        "- we also have to calculate gradients for the recurrent weights $U$;  \n",
        "- because we reuse the same weights multiple times in recurrence, we end up with multiple gradients for those weights, which we have to combine through addition.  \n",
        "\n",
        "The process is laid out below:\n",
        "\n",
        "*Note: in this notation, we're using $\\mathbf{\\varepsilon}$ for the local gradient / error of the output layer, and $\\mathbf{\\delta}_t$ for the local gradient of the hidden layer at timestep $t$. We're also using $\\ast$ to represent element-wise multiplication (the `*` operator in NumPy).*\n",
        "\n",
        "#### RNN CLASSIFIER BACKPROP STEPS\n",
        "\n",
        "1. calculate the *local gradient* of the output layer $\\mathbf{\\varepsilon}$ from the *error* in $p$;  \n",
        "2. calculate the *output weights gradient*, based on the hidden layer activations at the final timestep: $\\Delta \\mathbf{v}^T = \\mathbf{\\varepsilon}\\mathbf{h}_n^T$;  \n",
        "3. calculate the *local gradient* of the hidden layer at timestep $n$, $\\mathbf{\\delta}_n$:  \n",
        "   1. send the local gradient backward through the output weights: $\\mathbf{s}_n = \\mathbf{v}\\mathbf{\\varepsilon}$  \n",
        "   2. multiply element-wise by the *activation derivative* of the hidden layer at timestep $n$: $\\mathbf{\\delta}_n = g^\\prime(\\mathbf{h}_n) \\ast \\mathbf{s}_n$  \n",
        "5. for each timestep $t$ counting down from $n$ to 2:  \n",
        "   1. calculate the *input weights gradient* at timestep $t$: $\\Delta W_t = \\mathbf{\\delta}_t \\mathbf{x}_t^T$\n",
        "   2. calculate the *recurrent weights gradient* at timestep $t$: $\\Delta U_t = \\mathbf{\\delta}_t \\mathbf{h}_{t-1}^T$  \n",
        "   3. send the local gradient backward to timestep $t-1$, through the recurrent weights: $\\mathbf{s}_{t-1} = U^T \\mathbf{\\delta}_t$  \n",
        "   4. multiply element-wise by the *activation derivative* of the hidden layer at timestep $t-1$: $\\mathbf{\\delta}_{t-1} = g^\\prime(\\mathbf{h}_{t-1}) \\ast \\mathbf{s}_{t-1}$  \n",
        "6. after back-propagating to timestep 1, calculate the *input weights gradient* at timestep 1: $\\Delta W_1 = \\mathbf{\\delta}_1 \\mathbf{x}_1^T$  \n",
        "7. calculate the *overall* weights gradients by adding together the gradients from different timesteps:  \n",
        "   1. for the overall *inputs weights gradient*: $\\Delta W = \\sum_{t=1}^{n} \\Delta W_t$  \n",
        "   2. for the overall *recurrent weights gradient*: $\\Delta U = \\sum_{t=2}^{n} \\Delta U_t$\n",
        "\n",
        "As in the forward pass, we assume that the bias terms are accounted for by the input weights $W$, so that there is no bias term in the recurrent weights $U$. We will point out how to deal with this assumption as we work through the activity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caaae93c-191b-4258-9e3a-94c7743734b9",
      "metadata": {
        "id": "caaae93c-191b-4258-9e3a-94c7743734b9"
      },
      "source": [
        "### Task 2.2.1: Getting the weights gradients at the current timestep [2 points]\n",
        "\n",
        "One of the key things that happens over and over in recurrent neural networks is getting the weights gradients for the input ($W$) and recurrence ($U$) weights at the current timestep ([RNN classifier backprop steps 4A-4B](#RNN-CLASSIFIER-BACKPROP-STEPS)), by multiplying the local gradient at that timestep ($\\delta_t$) by the (transpose of the) input ($\\mathbf{x}_t^T$) and previous hidden layer activations ($\\mathbf{h}_{t-1}^T$), respectively.\n",
        "\n",
        "Since we assume that the bias terms are accounted for by the input weights $W$, we have to make sure that we insert a value for the bias term in the input values vector ($\\mathbf{x}_t^T$) when calculating the input weights gradient. We do **not** insert a value for the bias term in the previous hidden layer activations ($\\mathbf{h}_{t-1}^T$) that are used in the calculation of the recurrence weights gradient, because the recurrence weights $U$ are assumed not to contain any bias terms.\n",
        "\n",
        "Write a function `get_weights_gradients()` that gets the weights gradients for the input and recurrent weights at the current timestep. Your function should task as arguments:  \n",
        "- the local gradient of the current timestep, `delta`, which is a vector of shape `(num_hidden,)`;  \n",
        "- a vector of `current_input` values (of shape `(num_features,)`); and  \n",
        "- a vector of `prev_hidden` activations (of shape `(num_hidden,)`). \n",
        "\n",
        "It should return a **tuple** with two elements, where:  \n",
        "- the first element is the `input_weights_gradient` (including a bias term); and  \n",
        "- the second element is the `recurrent_weights_gradient` (without a bias term) at this timestep.\n",
        "\n",
        "*Note: you should use the `calculate_weights_gradient()` function that we have provided. This function contains a keyword argument `insert_bias_term` to control whether or not to insert a value for the bias term (which is set to `True` by default); be sure to set `insert_bias_term=False` when you are calculating the weights gradient for the recurrent weights $U$, because the recurrent weights matrix does not contain a bias term*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8de38aa-b41b-4029-bf30-057b8d8b56db",
      "metadata": {
        "id": "a8de38aa-b41b-4029-bf30-057b8d8b56db"
      },
      "outputs": [],
      "source": [
        "def get_weights_gradients(delta, current_input, prev_hidden):\n",
        "    \"\"\"Gets the input and recurrent weights gradients for a given timestep of\n",
        "    a recurrent neural network.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    delta: np.array representing a vector of size (num_hidden,),\n",
        "           containing the local gradients of the hidden layer \n",
        "           at the current timestep (t)\n",
        "    current_input: np.array representing a vector of size (num_features,),\n",
        "                   containing the values of input features at the current timestep (t).\n",
        "                   Does not contain a value for the bias term\n",
        "    prev_hidden: np.array representing a vector of size (num_hidden,);\n",
        "                 represents the hidden layer activations from the previous timestep (t-1).\n",
        "                 Does not contain a value for the bias term. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple (input_weights_gradient, recurrent_weights_gradient), where:\n",
        "             input_weights_gradient: np.array representing a matrix of size (num_hidden, num_features + 1)\n",
        "                                     which contains the gradient of the loss function with respect to the \n",
        "                                     input weights at the current timestep (t).\n",
        "                                     The first element in each row corresponds to the bias term.\n",
        "             recurrent_weights_gradient: np.array representing a matrix of size (num_hidden, num_hidden)\n",
        "                                         which contains the gradient of the loss function with respect to \n",
        "                                         the recurrent weights at the current timestep (t).\n",
        "                                         Does not contain a bias term.\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a89428c-696f-4db5-8cc0-eb7d5cce61f7",
      "metadata": {
        "id": "4a89428c-696f-4db5-8cc0-eb7d5cce61f7"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b4fb2b1-f7ac-40ab-bbda-25bb5ae90457",
      "metadata": {
        "id": "6b4fb2b1-f7ac-40ab-bbda-25bb5ae90457"
      },
      "outputs": [],
      "source": [
        "check(get_weights_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c7264e2-d807-46ec-96f7-e4de4b6e85dd",
      "metadata": {
        "id": "7c7264e2-d807-46ec-96f7-e4de4b6e85dd"
      },
      "source": [
        "### Task 2.2.2: Getting the local gradient at the previous timestep [2 points]\n",
        "\n",
        "The other key thing that happens over and over in recurrent neural networks is getting the local gradient at the previous timestep, by sending the current local gradient ($\\delta_t$) back through the recurrent weights ($U^T$) and multiplying it element-wise by the activation derivatives of the hidden layer at the previous timestep ($g^\\prime(\\mathbf{h}_{t-1})$) ([RNN classifier backprop steps 4C-4D](#RNN-CLASSIFIER-BACKPROP-STEPS)).\n",
        "\n",
        "Since we assume that there is no bias term in the recurrent weights matrix $U$ (because it is already accounted for by the input weights $W$), we do **not** have to remove the bias term when sending the local gradient back through the recurrence weights like we do when sending it back through the output weights ($\\mathbf{v}$).\n",
        "\n",
        "Write a function `get_prev_delta()` that gets the local gradient at the previous timestep. Your function should take as arguments:  \n",
        "- the local gradient of the current timestep, `delta`, which is a vector of shape `(num_hidden,)`;  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a vector of `prev_hidden` activations (of shape `(num_hidden,)`); and  \n",
        "- an `activation_derivative` function (which is `relu_derivative` by default). \n",
        "\n",
        "It should return a vector `prev_delta` (of shape `(num_hidden,)`) containing the local gradient of the previous timestep.\n",
        "\n",
        "*Note: you should use the `send_delta_back()` function that we have provided. This function contains a keyword argument `remove_bias_term` to control whether or not to remove the bias term from the weights matrix when sending the local gradient back through that weights matrix (which is set to `True` by default); be sure to set `remove_bias_term=False` when you are calculating the local gradient at the previous timestep, because the recurrent weights matrix does not contain a bias term*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b6b441-e32a-49df-9100-8c86594321c1",
      "metadata": {
        "id": "b9b6b441-e32a-49df-9100-8c86594321c1"
      },
      "outputs": [],
      "source": [
        "def get_prev_delta(delta, recurrent_weights, prev_hidden, \n",
        "                   activation_derivative=relu_derivative):\n",
        "    \"\"\"Gets the local gradient at the previous timestep.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    delta: np.array representing a vector of size (num_hidden,),\n",
        "           containing the local gradients of the hidden layer \n",
        "           at the current timestep (t)\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    prev_hidden: np.array representing a vector of size (num_hidden,);\n",
        "                 represents the hidden layer activations from the previous timestep (t-1).\n",
        "                 Does not contain a value for the bias term. \n",
        "    activation_derivative: function; a function that calculates the activation derivative\n",
        "                           of the hidden layer (default is derivative of ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    prev_delta: np.array representing a vector of shape (num_hidden,),\n",
        "                containing the local gradients of the hidden layer \n",
        "                at the previous timestep (t-1)\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc38b609-1c5c-4341-9630-51db1c2a096f",
      "metadata": {
        "id": "bc38b609-1c5c-4341-9630-51db1c2a096f"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41a1b8b-bd51-41a5-b801-5fdef0e63e7c",
      "metadata": {
        "id": "c41a1b8b-bd51-41a5-b801-5fdef0e63e7c"
      },
      "outputs": [],
      "source": [
        "check(get_prev_delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c476d15-78c9-4bd5-8e52-0f70556d7580",
      "metadata": {
        "id": "7c476d15-78c9-4bd5-8e52-0f70556d7580"
      },
      "source": [
        "### Task 2.2.3: Getting the overall weights gradients for input and recurrent weights [5 points]\n",
        "\n",
        "The primary way in which backprop through time differs from regular backprop (in feedfoward neural networks) is that it accumulates weights gradients for the input and recurrent weights across timesteps ([RNN classifier backprop steps 4-6](#RNN-CLASSIFIER-BACKPROP-STEPS)). For the input weights gradient ($\\Delta W$), this means adding together the $n$ components obtained at timesteps 1 through $n$ ($\\Delta W_1, \\cdots, \\Delta W_n$). For the recurrent weights gradient ($\\Delta U$), it means adding together the $(n-1)$ components obtained at timesteps 2 through $n$ ($\\Delta U_2, \\cdots, \\Delta U_n$) (there is no component of the recurrent weights gradient at timestep 1 because there is no previous hidden layer that provides input to the hidden layer at timestep 1).\n",
        "\n",
        "Write a function `get_overall_weights_gradients()` that propagates the hidden layer local gradient ($\\delta_t$) backward in time from timestep $n$ to timestep 1, accumulating the input and recurrent weights gradients across all timesteps along the way. Your function should take as arguments:  \n",
        "- a vector `delta` of the hidden layer local gradient at timestep $n$ (of shape `(num_hidden,)`);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a list `values_list` of the values vectors that are input to the model at each timestep, in order from timestep 1 to $n$ (each a vector of shape `(num_features,)`, which does not contain a value for the bias term);  \n",
        "- a list `prev_hidden_list` of the hidden layer activations at all *previous* timesteps, in order from timestep 1 to $n-1$ (each a vector of shape `(num_hidden,)`; and  \n",
        "- an `activation_derivative` function (which is `relu_derivative` by default). \n",
        "\n",
        "It should return a **tuple** with two elements, where:  \n",
        "- the first element is the `input_weights_gradient` (including a bias term), accumulated over timesteps 1 to $n$; and  \n",
        "- the second element is the `recurrent_weights_gradient` (without a bias term), accumulated over timesteps 2 to $n$.\n",
        "\n",
        "In the event that the input sequence has a *single* timestep, you should return `None` for the `recurrent_weights_gradient`.\n",
        "\n",
        "**Important:** You will need to work backward through the timesteps. For timesteps from $n$ to 2, you can use your `get_weights_gradient()` function to get both the input and recurrent weights gradients at that timestep. For timestep 1, since there is no recurrent weights gradient, you will have to separately calculate the input weights gradient, using the `calculate_weights_gradient()` function with the default value of `insert_bias_term=True` (because the input weights have an entry for the bias term, but the input values vector does not).\n",
        "\n",
        "Note: we have taken a *copy* of some of the arguments at the beginning of the code, to prevent them being changed due to mutability issues. This is important for being able to re-use functions later on.\n",
        "\n",
        "*Hint: don't forget to pass the `activation_derivative` argument to `get_prev_delta()` when you use it, as in `get_prev_delta(..., activation_derivative=activation_derivative)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b274d6-c433-4a25-8537-1a41cf5c4da4",
      "metadata": {
        "id": "17b274d6-c433-4a25-8537-1a41cf5c4da4"
      },
      "outputs": [],
      "source": [
        "def get_overall_weights_gradients(delta, recurrent_weights, values_list, prev_hidden_list,\n",
        "                                  activation_derivative=relu_derivative):\n",
        "    \"\"\"Gets the overall input and recurrent weights gradients, accumulated over all timesteps\n",
        "    from the error at the final timestep.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    delta: np.array representing a vector of size (num_hidden,), containing the \n",
        "           local gradients of the hidden layer at the final timestep (n)\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    values_list: list of n np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector\n",
        "                 of input values for a particular timestep (ordered from 1 to n).\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    prev_hidden_list: a list of (n-1) np.arrays, each representing a vector of size (num_hidden,);\n",
        "                      represents the sequence of hidden layer activations at all previous timesteps\n",
        "                      from 1 to (n-1), where each entry in the list is a vector of hidden layer\n",
        "                      activations for a particular timestep (ordered from 1 to n-1)\n",
        "    activation_derivative: function; a function that calculates the activation derivative\n",
        "                           of the hidden layer (default is derivative of ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple (input_weights_gradient, recurrent_weights_gradient), where:\n",
        "             input_weights_gradient: np.array representing a matrix of size (num_hidden, num_features + 1)\n",
        "                                     which contains the gradient of the loss function with respect to the \n",
        "                                     input weights, accumulated over timesteps 1 to n.\n",
        "                                     The first element in each row corresponds to the bias term.\n",
        "             recurrent_weights_gradient: np.array representing a matrix of size (num_hidden, num_hidden)\n",
        "                                         which contains the gradient of the loss function with respect to \n",
        "                                         the recurrent weights, accumulated over timesteps 2 to n.\n",
        "                                         Does not contain a bias term.\n",
        "    \"\"\"\n",
        "    # =====================================================\n",
        "    # DO NOT CHANGE THIS PART OR WRITE ABOVE IT\n",
        "    values_list = copy.deepcopy(values_list)\n",
        "    prev_hidden_list = copy.copy(prev_hidden_list)\n",
        "    # =====================================================\n",
        "    \n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76de92ef-f110-45c8-81a2-d2f05f5b7c85",
      "metadata": {
        "id": "76de92ef-f110-45c8-81a2-d2f05f5b7c85"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a7a461-3536-42b8-bc91-62eca4f76baf",
      "metadata": {
        "id": "a4a7a461-3536-42b8-bc91-62eca4f76baf"
      },
      "outputs": [],
      "source": [
        "check(get_overall_weights_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11602424-94a2-41e7-b4b7-fdcc123e75cf",
      "metadata": {
        "id": "11602424-94a2-41e7-b4b7-fdcc123e75cf"
      },
      "source": [
        "### Task 2.2.4: The complete backward pass [3 points]\n",
        "\n",
        "So far, we have calculated the input and recurrent weights gradients, given the local gradient at the final hidden layer (timestep $n$); this corresponds to [RNN classifier backprop steps 4-6](#RNN-CLASSIFIER-BACKPROP-STEPS). To put everything together for the complete backward pass, we need to use the error at the output layer to calculate the output weights gradient ([RNN classifier backprop steps 1-2](#RNN-CLASSIFIER-BACKPROP-STEPS)), and then propagate this error backward to derive the local gradient at the final hidden layer ([RNN classifier backprop step 3](#RNN-CLASSIFIER-BACKPROP-STEPS)). This will let us continue on to use the `get_overall_weights_gradients()` function already written.\n",
        "\n",
        "Write a function `recurrent_classifier_backward()` which performs a complete backward pass ([RNN classifier backprop steps 1-6](#RNN-CLASSIFIER-BACKPROP-STEPS)). Your function should take as arguments:  \n",
        "- the `predicted` probability output by the model for an input sequence;  \n",
        "- an integer representing the `actual` class for the sequence;  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first entry represents the bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a list `values_list` of the values vectors that are input to the model at each timestep, in order from timestep 1 to $n$ (each a vector of shape `(num_features,)`, which does not contain a value for the bias term);  \n",
        "- a list `hidden_list` of the hidden layer activations at all timesteps, in order from timestep 1 to $n$ (each a vector of shape `(num_hidden,)`; and  \n",
        "- an `activation_derivative` function (which is `relu_derivative` by default). \n",
        "\n",
        "It should return a **tuple** with *three* elements, where:  \n",
        "- the first element is the `input_weights_gradient` (including a bias term), accumulated over timesteps 1 to $n$;  \n",
        "- the second element is the `recurrent_weights_gradient` (without a bias term), accumulated over timesteps 2 to $n$; and  \n",
        "- the third element is the `output_weights_gradient` (including a bias term), which is calculated only at timestep $n$.\n",
        "\n",
        "**Important:** For the output weights gradient, you should use the `calculate_weights_gradient()` function with the default value of `insert_bias_term=True` (because the output weights have an entry for the bias term, but the hidden layer activations vector does not).\n",
        "\n",
        "Note: we have taken a *copy* of some of the arguments at the beginning of the code, to prevent them being changed due to mutability issues. This is important for being able to re-use functions later on.\n",
        "\n",
        "*Hint: you can use the function `crossent_loss_gradient()` to calculate the local gradient (error) at the output layer*\n",
        "\n",
        "*Hint: when back-propagating the local gradient at the output layer to the local gradient at the hidden layer, make sure you use `activation_derivative()` to represent the activation derivative function, rather than e.g. `relu_derivative()`. Also don't forget to pass the `activation_derivative` argument to `get_overall_weights_gradient()` when you use it, as in `get_overall_weights_gradient(..., activation_derivative=activation_derivative)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb55f81-ae5f-4b7f-93a6-7f820f014731",
      "metadata": {
        "id": "5bb55f81-ae5f-4b7f-93a6-7f820f014731"
      },
      "outputs": [],
      "source": [
        "def recurrent_classifier_backward(predicted, actual, \n",
        "                                  output_weights, recurrent_weights, \n",
        "                                  values_list, hidden_list,\n",
        "                                  activation_derivative=relu_derivative):\n",
        "    \"\"\"Back-propagates errors through a 2-layer recurrent neural network classifier,\n",
        "    to determine the gradients of the input, recurrent, and output weights.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    predicted: float; the probability output by the classifier for a given input sequence\n",
        "    actual: int; the actual class for the sequence (0 for default, 1 for alternative)\n",
        "    output_weights: np.array representing a vector of shape (num_hidden + 1,);\n",
        "                    represents the weights between the network's final hidden layer\n",
        "                    (at timestep n) and the output layer.\n",
        "                    The first element corresponds to the bias term.\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    values_list: list of n np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector\n",
        "                 of input values for a particular timestep (ordered from 1 to n).\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    hidden_list: a list of n np.arrays, each representing a vector of size (num_hidden,);\n",
        "                 represents the sequence of hidden layer activations at all timesteps, \n",
        "                 where each entry in the list is a vector of hidden layer activations \n",
        "                 for a particular timestep (ordered from 1 to n)\n",
        "    activation_derivative: function; a function that calculates the activation derivative\n",
        "                           of the hidden layer (default is derivative of ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple (input_weights_gradient, recurrent_weights_gradient, output_weights_gradient), where:\n",
        "             input_weights_gradient: np.array representing a matrix of size (num_hidden, num_features + 1)\n",
        "                                     which contains the gradient of the loss function with respect to the \n",
        "                                     input weights, accumulated over timesteps 1 to n.\n",
        "                                     The first element in each row corresponds to the bias term.\n",
        "             recurrent_weights_gradient: np.array representing a matrix of size (num_hidden, num_hidden)\n",
        "                                         which contains the gradient of the loss function with respect to \n",
        "                                         the recurrent weights, accumulated over timesteps 2 to n.\n",
        "                                         Does not contain a bias term.\n",
        "             output_weights_gradient: np.array representing a vector of shape (num_hidden + 1,),\n",
        "                                      which contains the gradient of the loss function with respect to the \n",
        "                                      output weights (calculated based on the final timestep).\n",
        "                                      The first element in each row corresponds to the bias term.\n",
        "    \"\"\"\n",
        "    # =====================================================\n",
        "    # DO NOT CHANGE THIS PART OR WRITE ABOVE IT\n",
        "    hidden_list = copy.copy(hidden_list)\n",
        "    # =====================================================\n",
        "    \n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b14a57f-f38d-4e36-baca-08f17eebc97f",
      "metadata": {
        "id": "9b14a57f-f38d-4e36-baca-08f17eebc97f"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4097c1a5-abab-41ac-bd79-5cac5309ec43",
      "metadata": {
        "id": "4097c1a5-abab-41ac-bd79-5cac5309ec43"
      },
      "outputs": [],
      "source": [
        "check(recurrent_classifier_backward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cadedfa-7777-4371-951b-47a20be904a2",
      "metadata": {
        "id": "9cadedfa-7777-4371-951b-47a20be904a2"
      },
      "source": [
        "## Activity 2.3: Training a sequence classifier\n",
        "\n",
        "Training a sequence classifier via Stochastic Gradient Descent works just like training any other neural network classifier, except that there are more weights to consider:  \n",
        "\n",
        "1. iterate over the training samples (in random order);  \n",
        "\n",
        "2. for each training sample consisting of input sequence $[\\mathbf{x}_1, \\cdots, \\mathbf{x}_n]$:  \n",
        "\n",
        "   1. complete a forward pass using the current input weights ($W$), recurrent weights ($U$), and output weights ($\\mathbf{v}^T$) to get the sequence of hidden layer activations $[\\mathbf{h}_1, \\cdots, \\mathbf{h}_n]$, as well as the classification probability $p$ output by the network;\n",
        "   \n",
        "   2. complete a backward pass that compares $p$ to the *actual* class for the training sample, and uses this comparison alongside the input ($[\\mathbf{x}_1, \\cdots, \\mathbf{x}_n]$) and hidden sequences ($[\\mathbf{h}_1, \\cdots, \\mathbf{h}_n]$) to derive (overall) weights gradients ($\\Delta W$, $\\Delta U$, $\\Delta \\mathbf{v}^T$);  \n",
        "      \n",
        "   3. update each set of weights by *subtracting* a nudge based on the corresponding weights gradient and the learning rate $\\eta$ *(NB: $\\leftarrow$ means \"is overwritten by\")*:  \n",
        "   \n",
        "      $$W \\leftarrow W - \\Delta W \\times \\eta$$  \n",
        "      $$U \\leftarrow U - \\Delta U \\times \\eta$$  \n",
        "      $$\\mathbf{v}^T \\leftarrow \\mathbf{v}^T - \\Delta\\mathbf{v}^T \\times \\eta$$  \n",
        "      \n",
        "3. repeat for a fixed number of epochs (entire cycles through the training data), or until convergence  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d58e5e6-e716-4d35-87d8-945c3760d6a5",
      "metadata": {
        "id": "4d58e5e6-e716-4d35-87d8-945c3760d6a5"
      },
      "source": [
        "### Task 2.3.1: Training for a single epoch [3 points]\n",
        "\n",
        "Write a function `train_epoch_classifier()` that performs steps 1-2 of the training outline above. Your function should takes as arguments:  \n",
        "- a list of (`values_list`, `actual_class`) pairs representing training samples, where `values_list` is a list of the values vectors that are input to the model at each timestep, arranged in order from timestep 1 to $n$ (each a vector of shape `(num_features,)`, which does not contain a value for the bias term);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first element corresponds to a bias term);  \n",
        "- a `learning_rate`; and  \n",
        "- the **name** of an activation function (a **string**, which is `\"relu\"` by default). \n",
        "\n",
        "It should iterate over the training samples, nudging the weights for each one, and then return the final weights after nudging once for every training sample.\n",
        "\n",
        "**Note:** We have provided code that uses the name you provide for an activation function (e.g. `\"relu\"`) to look up corresponding *functions* to use for hidden layer activations and derivatives (e.g. `relu()` and `relu_derivative()`). These functions are stored in the variables `activation_function` and `activation_derivative`, respectively; make sure that you pass these variables as arguments to `recurrent_classifier_forward()` and `recurrent_classifier_backward()`, as in `recurrent_classifier_forward(..., activation_function=activation_function)` and `recurrent_classifier_backward(..., activation_derivative=activation_derivative)`. We have also taken a *copy* of some of the arguments at the beginning of the code, to prevent them being changed due to mutability issues. This is important for being able to re-use functions later on.\n",
        "\n",
        "**Note:** You may assume that the training samples have already been shuffled into a random order.\n",
        "\n",
        "*Hint: Make sure that your nudges stack up, by overwriting the weights with their new values after each training sample.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4038836-2950-4972-9b71-b711618766f4",
      "metadata": {
        "id": "b4038836-2950-4972-9b71-b711618766f4"
      },
      "outputs": [],
      "source": [
        "def train_epoch_classifier(train_samples, input_weights, recurrent_weights, \n",
        "                           output_weights, learning_rate, activation_name=\"relu\"):\n",
        "    \"\"\"Applies a single epoch of SGD to a RNN sequence classifier,\n",
        "    and returns the resultant new weights.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    train_samples: list of pairs (values, actual_class), where each pair\n",
        "                   corresponds to a single training sample for which:\n",
        "                   values is a list of input values vectors (without bias term), \n",
        "                          one for each timestep; and\n",
        "                   actual_class is an integer representing the actual class.\n",
        "    input_weights: np.array of weights between the input and hidden layer, \n",
        "                   including a weight for the bias term.\n",
        "    recurrent_weights: np.array of weights between hidden layers at successive\n",
        "                       timepoints, with no weight for the bias term.\n",
        "    output_weights: np.array of weights between the final hidden layer and the\n",
        "                    output, including a weight for the bias term.\n",
        "    learning_rate: float; the learning rate, eta\n",
        "    activation_name: str; the name of an activation function, for which there is\n",
        "                     a corresponding activation derivative function with the suffix\n",
        "                     _derivative\n",
        "                     \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple (new_input_weights, new_recurrent_weights, new_output_weights),\n",
        "             where each set of weights has had a single nudge update for each\n",
        "             training sample\n",
        "    \"\"\"\n",
        "    # ===========================================================\n",
        "    # DO NOT CHANGE THIS PART OR WRITE ABOVE IT\n",
        "    activation_function = eval(activation_name)\n",
        "    activation_derivative = eval(activation_name + \"_derivative\")\n",
        "    input_weights = copy.copy(input_weights)\n",
        "    recurrent_weights = copy.copy(recurrent_weights)\n",
        "    output_weights = copy.copy(output_weights)\n",
        "    # ===========================================================\n",
        "    \n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "392fdfd1-494b-455e-aaab-a0c0239f66b2",
      "metadata": {
        "id": "392fdfd1-494b-455e-aaab-a0c0239f66b2"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d385ca-6c01-4ff0-b49a-b526b7da96ef",
      "metadata": {
        "id": "c7d385ca-6c01-4ff0-b49a-b526b7da96ef"
      },
      "outputs": [],
      "source": [
        "check(train_epoch_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1d418f-6736-442d-9169-41d1c34da9ab",
      "metadata": {
        "id": "9e1d418f-6736-442d-9169-41d1c34da9ab"
      },
      "source": [
        "### Task 2.3.2: Tracking the loss [3 points]\n",
        "\n",
        "As our classifier is trained, we expect its loss to decrease. In order to be able to assess whether this is true, we need to be able to calculate the loss over a set of (training or test) samples.\n",
        "\n",
        "Even though the inputs here are sequences (rather than single vectors as in logistic regression and feedforward neural networks), the network is still only doing a single binary classification for each input (sequence). Therefore, loss is calculated in exactly the same way as it is in logistic regression and feedforward neural networks. For a given training sample, the network outputs a probability $p$. The binary cross-entropy loss associated with this output is:\n",
        "\n",
        "  $$L = \\begin{cases}\n",
        "  -\\log(p) & \\text{if actual class is 1 (alternative)} \\\\\n",
        "  -\\log(1-p) & \\text{if actual class is 0 (default)}\n",
        "  \\end{cases}\n",
        "  $$\n",
        "  \n",
        "The overall loss of the network on a given set of (training or test) samples is the average of the losses for each individual sample.\n",
        "\n",
        "Write a function `recurrent_classifier_loss()` that calculates the overall binary cross-entropy loss of a recurrent neural network classifier for a given set of (training or test) samples. Your function should take as arguments:  \n",
        "- a list of (`values_list`, `actual_class`) pairs representing (training or test) samples, where `values_list` is a list of the values vectors that are input to the model at each timestep, arranged in order from timestep 1 to $n$ (each a vector of shape `(num_features,)`, which does not contain a value for the bias term);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element in each row corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first element corresponds to a bias term); and  \n",
        "- an activation function (which is `relu` by default).\n",
        "\n",
        "It should return a float representing the average of the losses for each individual sample.\n",
        "\n",
        "**Important:** `recurrent_classifier_loss()` also has an argument `eps`, which is set to a small value. You should use `eps` to adjust the value you are taking the log of when calculating the loss for a sample: if this value is lower than `eps`, you should instead replace it by `eps` prior to taking the log. This prevents numerical instability caused by the fact that `log(0)` is undefined.\n",
        "\n",
        "*Note: you should use the function `np.log()` to take logs.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb61142-c890-4cd1-a5c8-842bc66c0f3c",
      "metadata": {
        "id": "beb61142-c890-4cd1-a5c8-842bc66c0f3c"
      },
      "outputs": [],
      "source": [
        "def recurrent_classifier_loss(samples, input_weights, recurrent_weights, \n",
        "                              output_weights, activation_function=relu, eps=1e-8):\n",
        "    \"\"\"Calculates the average loss of a RNN classifier over a set of samples.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    samples: list of pairs (values_list, actual_class), where each pair\n",
        "             corresponds to a single training sample for which:\n",
        "             values_list is list of input values vectors (without bias\n",
        "             term), one for each timestep;\n",
        "             and actual_class is an integer representing the actual class.\n",
        "    input_weights: np.array of weights between the input and hidden layer, \n",
        "                   including a weight for the bias term.\n",
        "    recurrent_weights: np.array of weights between hidden layers at successive\n",
        "                       timepoints, with no weight for the bias term.\n",
        "    output_weights: np.array of weights between the final hidden layer and the\n",
        "                    output, including a weight for the bias term.\n",
        "    activation_function: function; the activation function for the hidden layer\n",
        "                         (default is ReLU)\n",
        "    eps: float; the minimum value that should be used as argument to np.log(),\n",
        "         to prevent numerical instabilities.\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719c6eb6-8dcc-4c3f-9702-0cb68187f43d",
      "metadata": {
        "id": "719c6eb6-8dcc-4c3f-9702-0cb68187f43d"
      },
      "source": [
        "To make sure that your function incorporates the `eps` argument correctly, run the following cell. If you haven't incorporated `eps` correctly, your function will return `inf` (and you might see a warning message). If you have incorporated `eps` correctly, your function will return ~18.4207."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ea91bd1-bf4e-4aeb-a077-17126aed8db2",
      "metadata": {
        "tags": [],
        "id": "6ea91bd1-bf4e-4aeb-a077-17126aed8db2"
      },
      "outputs": [],
      "source": [
        "samples = [([np.array([2, 0]), np.array([0, 1])], 0), \n",
        "           ([np.array([1, 0]), np.array([0, 2])], 1)]\n",
        "input_weights = np.array([[0, 1, 0], [0, 0, 1]])\n",
        "recurrent_weights = np.array([[1, 0], [0, 1]])\n",
        "output_weights = np.array([0, 100, -100])\n",
        "recurrent_classifier_loss(samples, input_weights, recurrent_weights, output_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49a14954-118e-4d96-a401-0e27a75e789f",
      "metadata": {
        "id": "49a14954-118e-4d96-a401-0e27a75e789f"
      },
      "source": [
        "To test your code, run the cell below. You should see the loss decrease from ~1.171 with original weights to ~0.756 after 1 epoch. This means training is helping!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3569188d-4cc1-4de8-bbc2-d70930a4c4aa",
      "metadata": {
        "id": "3569188d-4cc1-4de8-bbc2-d70930a4c4aa"
      },
      "outputs": [],
      "source": [
        "train_samples = [\n",
        "    ([np.array([1, 2, 3], dtype=\"float64\"),\n",
        "      np.array([0, 1, 1], dtype=\"float64\")],\n",
        "     1),\n",
        "    ([np.array([1, 1, 0], dtype=\"float64\"),\n",
        "      np.array([2, 0, 1], dtype=\"float64\")],\n",
        "     0),\n",
        "    ([np.array([1, 2, 0], dtype=\"float64\"),\n",
        "      np.array([-1, 1, 0], dtype=\"float64\")],\n",
        "     1)\n",
        "]\n",
        "input_weights = np.array([[-1, 1, 0, 0], [-1, 0, 1, 0]], dtype=\"float64\")\n",
        "recurrent_weights = np.array([[0, 1], [-1, 1]], dtype=\"float64\")\n",
        "output_weights = np.array([1, 1, -1], dtype=\"float64\")\n",
        "learning_rate = 0.1\n",
        "\n",
        "trained_weights = train_epoch_classifier(train_samples, input_weights, recurrent_weights, \n",
        "                                         output_weights, learning_rate)\n",
        "\n",
        "untrained_loss = recurrent_classifier_loss(train_samples, input_weights, recurrent_weights, output_weights)\n",
        "print(\"Loss with original weights: {}\".format(untrained_loss))\n",
        "\n",
        "trained_loss = recurrent_classifier_loss(train_samples, *trained_weights)\n",
        "print(\"Loss after 1 epoch: {}\".format(trained_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbcdac94-3703-486e-8dee-6a0948f55345",
      "metadata": {
        "id": "dbcdac94-3703-486e-8dee-6a0948f55345"
      },
      "source": [
        "## Activity 2.4: The forward pass in a sequence tagger / RNN language model\n",
        "\n",
        "The previous activities focused on a *sequence classifier*, where a sequence is classified in its entirety. Examples of this include sentiment analysis and spam detection; we don't want to classify the *individual words*, but rather the *whole document* which is composed of a sequence of words.\n",
        "\n",
        "An alternative structure we have seen in class is that of a *sequence tagger* or *RNN language model*. In this case, words are provided as input sequentially, one at a time, but a classification / prediction is made *after each input word* rather than waiting until all words have been provided. Examples of this include POS tagging (where we want a POS tag for each word in a sentence) and language modeling (where we want to predict the next word at each point of a sentence).\n",
        "\n",
        "In the remaining activities, you will see how the sequence classification setup extends straightforwardly to this sequence tagging / language modeling setup. There are two main adjustments:  \n",
        "1. there is an output at each timestep, rather than just at the final timestep [this is the big change]; and  \n",
        "2. the classification task is multi-choice or *multinomial* (using the *softmax* function) rather than yes-no or *binary* (using the *logistic* function) [this is a smaller but still important change].\n",
        "\n",
        "Here is a visualization of the model structure:\n",
        "\n",
        "![RNN sequence tagger structure](https://github.com/ucsb-ling111/assignment2-giovani-gutierrez/blob/main/images/rnn_tagger.png?raw=1)\n",
        "\n",
        "With these changes, the forward pass looks as follows:\n",
        "\n",
        "*Note: the output weights are now a matrix $V$, and the output of the network at each timestep is now a vector of probabilities, $\\mathbf{p}$*\n",
        "\n",
        "#### RNN TAGGER CALCULATION STEPS\n",
        "\n",
        "1. calculate the input to the hidden layer activation function at timestep 1, based on the input values at timestep 1: $\\mathbf{z}_1$ = $W\\mathbf{x}_1$\n",
        "2. calculate the hidden layer activations at timestep 1: $\\mathbf{h}_1$ = $g(\\mathbf{z}_1)$  \n",
        "3. calculate the input to the softmax (output) layer at timestep 1, based on the hidden layer activations: $\\mathbf{y}_1 = V\\mathbf{h}_1$  \n",
        "4. pass these inputs through the softmax function in order to get the output probabilities at timestep 1:  \n",
        "      $$\\mathbf{p}_1 = \\frac{\\exp(\\mathbf{y_1})}{\\sum\\exp(\\mathbf{y_1})}$$\n",
        "5. for each timestep $t$ from 2 to $n$:  \n",
        "   1. calculate the input to the hidden layer activation function at that timestep, based on the input values at that timestep and the hidden layer activations at the previous timestep: $\\mathbf{z}_t$ = $W\\mathbf{x}_t + U\\mathbf{h}_{t-1}$\n",
        "   2. calculate the hidden layer activations at that timestep: $\\mathbf{h}_t$ = $g(\\mathbf{z}_t)$  \n",
        "   3. calculate the input to the softmax (output) layer at that timestep, based on the hidden layer activations: $\\mathbf{y}_t = V\\mathbf{h}_t$  \n",
        "   4. pass these inputs through the softmax function in order to get the output probabilities at that timestep:  \n",
        "         $$\\mathbf{p}_t = \\frac{\\exp(\\mathbf{y_t})}{\\sum\\exp(\\mathbf{y_t})}$$\n",
        "\n",
        "*Note: we assume that bias terms are built into $W$ and $V$ but not $\\mathbf{x}_t$ and $\\mathbf{h}_t$; as before, this requires some manipulation in order to work everything out*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "567b979e-759d-4e39-917b-a8541d73ded1",
      "metadata": {
        "id": "567b979e-759d-4e39-917b-a8541d73ded1"
      },
      "source": [
        "### Task 2.4.1: The softmax layer [2 points]\n",
        "\n",
        "Write a function `softmax_layer()` that represents a softmax layer of a neural network. Your function should take as arguments:  \n",
        "- a vector of hidden activation `values` (of shape `(num_hidden,)`, with no value for the bias term); and  \n",
        "- a matrix of output `weights` (of shape `(num_outputs, num_hidden + 1)`, where the first element in each row is the bias term). \n",
        "\n",
        "It should return the result of multiplying the values by the weights and sending the result through a softmax function ([RNN tagger calculation steps 3-4](#RNN-TAGGER-CALCULATION-STEPS)).\n",
        "\n",
        "**Note:** as you did in Assignment 1 for `logistic_layer()`, you will have to ensure that you insert a bias term at the beginning of `values` before you multiply it by the `weights`.\n",
        "\n",
        "*Note: you can use the `linear()` function we have provided to do the multiplication by weights. For taking exponentials in the softmax, use `np.exp()`; for taking the sum over the exponentiated values, use `exponentiated.sum()` (where `exponentiated` is the name of a variable containing a NumPy array of exponentiated values).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abb12469-bd39-41d3-a0a7-80f54c73dfdd",
      "metadata": {
        "id": "abb12469-bd39-41d3-a0a7-80f54c73dfdd"
      },
      "outputs": [],
      "source": [
        "def softmax_layer(values, weights):\n",
        "    \"\"\"Calculates the result of passing a set of values through a softmax\n",
        "    layer of a neural network with given weights.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    values: np.array representing a vector of size (num_hidden,);\n",
        "            does not contain a value for the bias term\n",
        "    weights: np.array representing a matrix of size (num_outputs, num_hidden + 1,);\n",
        "             the first element in each row corresponds to the bias term\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    activations: a vector of size (num_outputs,) where the values are probabilities\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9901e6a2-6b83-4bb8-ad8e-0e07264a2d49",
      "metadata": {
        "id": "9901e6a2-6b83-4bb8-ad8e-0e07264a2d49"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a3717b-b1ed-41ee-a9d8-c0e8a6da592e",
      "metadata": {
        "id": "26a3717b-b1ed-41ee-a9d8-c0e8a6da592e"
      },
      "outputs": [],
      "source": [
        "check(softmax_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d08eecbe-13d6-499f-b573-52a3b501628c",
      "metadata": {
        "id": "d08eecbe-13d6-499f-b573-52a3b501628c"
      },
      "source": [
        "### Task 2.4.2: Getting outputs at all timesteps [3 points]\n",
        "\n",
        "For the RNN sequence classifier, you wrote a function `get_hidden_sequence()` that gets the hidden layer activations at all timesteps. For the RNN tagger, we will need an additional function that uses these hidden layer activations to calculate the network's output at all timesteps.\n",
        "\n",
        "Write a function `get_output_sequence()` that calculates the outputs at each timestep of an RNN tagger, based on the hidden layer activations at those timesteps. Your function should take two arguments:  \n",
        "- a list of hidden layer activation vectors `hidden_list`, containing $n$ vectors each of shape `(num_hidden,)` (without a bias term), arranged in order from timestep 1 to timestep $n$; and  \n",
        "- a matrix of output `weights` of shape `(num_outputs, num_hidden + 1)` (where the first element in each row is the bias term). \n",
        "\n",
        "It should return a list of output vectors `output_list`, containing $n$ vectors each of shape `(num_outputs,)`, arranged in a corresponding order to `hidden_list`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65daca8e-7740-4905-b943-1747a8c478ea",
      "metadata": {
        "id": "65daca8e-7740-4905-b943-1747a8c478ea"
      },
      "outputs": [],
      "source": [
        "def get_output_sequence(hidden_list, weights):\n",
        "    \"\"\"Gets the sequence of outputs corresponding to a sequence of hidden layer\n",
        "    activations.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    hidden_list: a list of vectors of hidden layer activations at each timestep, \n",
        "                 each of shape (num_hidden,);\n",
        "                 these do not contain values for the bias term.\n",
        "    weights: np.array representing a matrix of size (num_outputs, num_hidden + 1,);\n",
        "             the first element in each row corresponds to the bias term\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    output_list: a list of vectors of network outputs at each timestep, \n",
        "                 each of shape (num_outputs,)\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d79656da-d2fb-40ed-b7a0-09838ccf65e0",
      "metadata": {
        "id": "d79656da-d2fb-40ed-b7a0-09838ccf65e0"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181334b8-70a9-453a-83ad-c44e18b79ce8",
      "metadata": {
        "id": "181334b8-70a9-453a-83ad-c44e18b79ce8"
      },
      "outputs": [],
      "source": [
        "check(get_output_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b44815-06e4-4756-b5f0-506f6bb77286",
      "metadata": {
        "id": "b8b44815-06e4-4756-b5f0-506f6bb77286"
      },
      "source": [
        "### Task 2.4.3: The complete forward pass [2 points]\n",
        "\n",
        "In the forward pass, the network calculates the hidden layer activations and outputs at each timestep. Both of these will be used in backprop.\n",
        "\n",
        "Write a function `recurrent_tagger_forward()` that calculates the hidden layer activations and outputs for each timestep of a 2-layer recurrent neural network, as described in the [RNN tagger calculation steps](#RNN-TAGGER-CALCULATION-STEPS). Your function should take as arguments:  \n",
        "- a list of values vectors `values_list` (where the entries in the list are vectors of shape `(num_features,)`, arranged in order from timestep 1 to timestep $n$);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first element corresponds to a bias term); and  \n",
        "- an `activation_function` (which is `relu` by default). \n",
        "\n",
        "It should return a **tuple** with two elements, where:  \n",
        "- the first element is the list of hidden layer activations for each timestep; and  \n",
        "- the second element is the list of outputs for each timestep.\n",
        "\n",
        "*Hint: use the `get_hidden_sequence()` function you wrote earlier, together with the `get_output_sequence()` function you wrote above. Don't forget to pass the `activation_function` argument to `get_hidden_sequence()`, as in `get_hidden_sequence(..., activation_function=activation_function)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bff8817-0899-4fd8-9e42-98bdce8f095a",
      "metadata": {
        "tags": [],
        "id": "1bff8817-0899-4fd8-9e42-98bdce8f095a"
      },
      "outputs": [],
      "source": [
        "def recurrent_tagger_forward(values_list, input_weights, recurrent_weights, output_weights, \n",
        "                             activation_function=relu):\n",
        "    \"\"\"Sends an input sequence forward through a 2-layer recurrent neural network\n",
        "    that generates an output at each timestep.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    values_list: list of np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector\n",
        "                 of input values for a particular timestep.\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    input_weights: np.array representing a matrix of size (num_hidden, num_features + 1);\n",
        "                   represents the weights between the network input and hidden layer.\n",
        "                   The first element in each row corresponds to the bias term.\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    output_weights: np.array representing a vector of shape (num_hidden + 1,);\n",
        "                    represents the weights between the final hidden layer and the output unit.\n",
        "                    The first element corresponds to the bias term.\n",
        "    activation_function: function; the activation function for the hidden layer\n",
        "                         (default is ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a pair (hidden_list, output_list), where\n",
        "            hidden_list: a list of vectors of hidden layer activations at each timestep, \n",
        "                         each of shape (num_hidden,)\n",
        "            outputs_list: a list of vectors of output probabilities at each timestep,\n",
        "                          each of shape (num_outputs,)\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb6352d-4b96-4e64-b793-bfbfcc226377",
      "metadata": {
        "id": "1bb6352d-4b96-4e64-b793-bfbfcc226377"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c125c5-2f23-49b2-a12d-e999ac822353",
      "metadata": {
        "id": "f5c125c5-2f23-49b2-a12d-e999ac822353"
      },
      "outputs": [],
      "source": [
        "check(recurrent_tagger_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48fd94a-3bc3-47e5-877a-d8c51b1c9ec0",
      "metadata": {
        "id": "b48fd94a-3bc3-47e5-877a-d8c51b1c9ec0"
      },
      "source": [
        "## Activity 2.5: The backward pass in a sequence tagger / RNN language model\n",
        "\n",
        "The two differences between the RNN tagger and the RNN sequence classifier have the following implications for the backward pass:  \n",
        "\n",
        "1. having an output at each timestep means that each timestep $\\tau$ generates its own error ($\\varepsilon^{\\langle\\tau\\rangle}$), and hence its own weights gradients ($\\Delta W^{\\langle\\tau\\rangle}$, $\\Delta U^{\\langle\\tau\\rangle}$, $\\Delta V^{\\langle\\tau\\rangle}$); we need to *average* the weights gradients generated from each timestep in order to get overall weights gradients for the sequence;  \n",
        "2. using a softmax output layer means that the errors at the output layer ($\\varepsilon^{\\langle\\tau\\rangle}$) form a *vector*, but are still back-propagated in exactly the same way as before.  \n",
        "\n",
        "The backprop steps for the RNN tagger are as follows:\n",
        "\n",
        "*Note: we are using subscripts $t$ to represent layers at timestep $t$, as before, and superscripts $\\langle\\tau\\rangle$ to represent values that are ultimately derived from errors in the output at timestep $\\tau$; thus, for example, $\\delta_3^{\\langle 5 \\rangle}$ represents the local gradient of the hidden layer at timestep 3, based on the error in the output at timestep 5.*\n",
        "\n",
        "*In the RNN sequence classifier we saw earlier, everything was derived from errors in the output at timestep $n$ (so you might imagine putting superscript $\\langle n \\rangle$ on $\\varepsilon$ and all the $\\delta_t$s and $\\mathbf{s}_t$s in the previous RNN classifier backprop steps).*\n",
        "\n",
        "#### RNN TAGGER BACKPROP STEPS\n",
        "\n",
        "1. for each *target timestep* $\\tau$ from 1 to $n$:  \n",
        "    1. calculate the *local gradient* of the output layer $\\mathbf{\\varepsilon}^{\\langle\\tau\\rangle}$ from the *error* in the output $\\mathbf{p}_\\tau$ at the target timestep $\\tau$;  \n",
        "    2. calculate the *intermediate output weights gradient* at the target timestep $\\tau$, based on the hidden layer activations at timestep $\\tau$: $\\Delta V^{\\langle\\tau\\rangle} = \\mathbf{\\varepsilon}^{\\langle\\tau\\rangle}\\mathbf{h}_\\tau^T$;  \n",
        "    3. calculate the *local gradient* of the hidden layer at timestep $\\tau$ based on the output error at timestep $\\tau$, $\\mathbf{\\delta}_\\tau^{\\langle\\tau\\rangle}$:  \n",
        "       1. send the local gradient backward through the output weights: $\\mathbf{s}_n^{\\langle\\tau\\rangle} = V^T\\mathbf{\\varepsilon}^{\\langle\\tau\\rangle}$  \n",
        "       2. multiply element-wise by the *activation derivative* of the hidden layer at timestep $\\tau$: $\\mathbf{\\delta}_\\tau^{\\langle\\tau\\rangle} = g^\\prime(\\mathbf{h}_\\tau) \\ast \\mathbf{s}_\\tau^{\\langle\\tau\\rangle}$  \n",
        "    5. for each timestep $t$ counting down from $\\tau$ to 2:  \n",
        "       1. calculate the *input weights gradient* at timestep $t$: $\\Delta W_t^{\\langle\\tau\\rangle} = \\mathbf{\\delta}_t^{\\langle\\tau\\rangle} \\mathbf{x}_t^T$\n",
        "       2. calculate the *recurrent weights gradient* at timestep $t$: $\\Delta U_t^{\\langle\\tau\\rangle} = \\mathbf{\\delta}_t^{\\langle\\tau\\rangle} \\mathbf{h}_{t-1}^T$  \n",
        "       3. send the local gradient backward to timestep $t-1$, through the recurrent weights: $\\mathbf{s}_{t-1}^{\\langle\\tau\\rangle} = U^T \\mathbf{\\delta}_t^{\\langle\\tau\\rangle}$  \n",
        "       4. multiply element-wise by the *activation derivative* of the hidden layer at timestep $t-1$: $\\mathbf{\\delta}_{t-1}^{\\langle\\tau\\rangle} = g^\\prime(\\mathbf{h}_{t-1}) \\ast \\mathbf{s}_{t-1}^{\\langle\\tau\\rangle}$  \n",
        "    6. after back-propagating to timestep 1, calculate the *input weights gradient* at timestep 1: $\\Delta W_1^{\\langle\\tau\\rangle} = \\mathbf{\\delta}_1^{\\langle\\tau\\rangle} \\mathbf{x}_1^T$  \n",
        "    7. calculate the *intermediate* weights gradients *generated from the target timestep $\\tau$* by adding together the gradients calculated at different timesteps:  \n",
        "       1. for the *intermediate inputs weights gradient*: $\\Delta W^{\\langle\\tau\\rangle} = \\sum_{t=1}^{\\tau} \\Delta W_t^{\\langle\\tau\\rangle}$  \n",
        "       2. for the *intermediate recurrent weights gradient*: $\\Delta U^{\\langle\\tau\\rangle} = \\sum_{t=2}^{\\tau} \\Delta U_t^{\\langle\\tau\\rangle}$\n",
        "2. after completing this process with all $n$ timesteps having been the *target timestep*, calculate the *overall* weights gradients *for the whole sequence* by *averaging* the intermediate weights gradients generated from each timestep:   \n",
        "    1. for the *overall inputs weights gradient*: $\\Delta W = \\frac{1}{n}\\sum_{\\tau=1}^{n} \\Delta W^{\\langle\\tau\\rangle}$  \n",
        "    2. for the *overall recurrent weights gradient*: $\\Delta U = \\frac{1}{n-1}\\sum_{\\tau=2}^{n} \\Delta U^{\\langle\\tau\\rangle}$  \n",
        "    3. for the *overall output weights gradient*: $\\Delta V = \\frac{1}{n}\\sum_{\\tau=1}^{n} \\Delta V^{\\langle\\tau\\rangle}$  \n",
        "\n",
        "Averaging ensures that long sequences do not dominate training just because they have more timesteps and therefore generate more intermediate weights gradients.\n",
        "\n",
        "*Note: we have $\\Delta U = \\frac{1}{n-1}\\sum_{\\tau=2}^{n} \\Delta U^{\\langle\\tau\\rangle}$ rather than $\\Delta U = \\frac{1}{n}\\sum_{\\tau=1}^{n} \\Delta U^{\\langle\\tau\\rangle}$ because there is no intermediate weights gradient for the recurrent weights at target timestep 1, since the recurrent weights are not involved in an RNN where the input sequence has only 1 timestep.*\n",
        "\n",
        "*Note: as before, we assume that the bias terms are accounted for by the input weights $W$, so that there is no bias term in the recurrent weights $U$.*\n",
        "\n",
        "This looks complicated! But it's actually basically just the same thing we did for the classifier model, applied over and over with errors generated at different timesteps: step 1 here contains all of the steps that we saw in the RNN sequence classifier, if we were to assume that the sequence ends at timestep $\\tau$. So if we break it down, it's not so bad!\n",
        "\n",
        "**Note:** We will be reusing functions from the RNN sequence classifier as much as possible, to highlight the commonalities. This means that our approach will not be as efficient as it could be, because it will be calculating values anew with each target timestep when it could be reusing them. Since we aren't training the model at scale, this is OK, and we think it is better for helping you understand."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4cc195-3304-473d-836a-43c2ab874513",
      "metadata": {
        "id": "bc4cc195-3304-473d-836a-43c2ab874513"
      },
      "source": [
        "### Task 2.5.1: Getting the intermediate weights gradients at all timesteps [5 points]\n",
        "\n",
        "The core of backprop for the RNN tagger involves getting the intermediate weights gradients for each target timestep $\\tau$ ([RNN tagger backprop step 1](#RNN-TAGGER-BACKPROP-STEPS)). For our purposes, these intermediate weights gradients are calculated in the same way as the weights gradients were in the `recurrent_classifier_backward()` function you wrote earlier, treating the target timestep $\\tau$ as the source of the error at the output layer. \n",
        "\n",
        "Write a function `get_intermediate_weights_gradients()` that calculates the intermediate weights gradients for each target timestep. Your function should take as arguments:  \n",
        "- a list `outputs_list` of $n$ output vectors (of shape `(num_outputs,)`), each representing the predicted probability distribution output by the softmax layer of the model at a particular target timestep (ordered from 1 to $n$);  \n",
        "- a list `actuals_list` of $n$ one-hot vectors (of shape `(num_outputs,)`), each representing the actual probabilities expected at the corresponding target timestep (where there is a 1 at the index of the actual class, and 0s elsewhere);  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first entry represents the bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a list `values_list` of the $\\tau$ values vectors that are input to the model at each of the $\\tau$ timesteps up to and including the target timestep (each a vector of shape `(num_features,)`, which does not contain a value for the bias term);  \n",
        "- a list `hidden_list` of the $\\tau$ hidden layer activations at each of the $\\tau$ timesteps up to and including the target timestep (each a vector of shape `(num_hidden,)`; and  \n",
        "- an `activation_derivative` function (which is `relu_derivative` by default). \n",
        "\n",
        "It should return a **tuple** containing **three lists**:  \n",
        "- the first is a list `input_weights_gradients_list` of the $n$ intermediate input weights gradients (each including a bias term);  \n",
        "- the second is a list `recurrent_weights_gradients_list` of the $n$ intermediate recurrent weights gradients (each without a bias term); and  \n",
        "- the third is a list `output_weights_gradients_list` of the $n$ intermediate output weights gradients (each including a bias term). \n",
        "\n",
        "In each of the returned lists, the first element will be the intermediate weights gradient treating timestep 1 as the target timestep (which should be `None` for the recurrent weights gradient), the second element will be the intermediate weights gradient treating timestep 2 as the target timestep, and so on and so forth.\n",
        "\n",
        "Note: we have taken a *copy* of some of the arguments at the beginning of the code, to prevent them being changed due to mutability issues. This is important for being able to re-use functions later on.\n",
        "\n",
        "*Hint: your function will contain a for loop over target timesteps, where on each iteration you slice the `values_list` and `hidden_list` up to and including the target timestep, look up the predicted and actual outputs for the target timestep from the `outputs_list` and `actuals_list`, respectively, and feed all of these things into `recurrent_classifier_backward()` to get the intermediate weights gradient at that timestep. When you use `recurrent_classifier_backward()`, make sure you pass the `activation_derivative` argument, as in `recurrent_classifier_backward(..., activation_derivative=activation_derivative)`.*\n",
        "\n",
        "*Hint: don't forget that the endpoint of a slice is not included in the slice!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9481260-026d-4e4f-8fc6-578a6202fc3b",
      "metadata": {
        "id": "d9481260-026d-4e4f-8fc6-578a6202fc3b"
      },
      "outputs": [],
      "source": [
        "def get_intermediate_weights_gradients(outputs_list, actuals_list, output_weights, \n",
        "                                       recurrent_weights, values_list, hidden_list,\n",
        "                                       activation_derivative=relu_derivative):\n",
        "    \"\"\"Calculates the intermediate weights gradients at all timesteps by back-propagating \n",
        "    errors from each target timestep through a 2-layer recurrent neural network tagger.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    outputs_list: list of n np.arrays of shape (num_outputs,), each representing the probability\n",
        "                  distribution output by the tagger at a particular target timestep \n",
        "                  (ordered from target timestep 1 to target timestep n)\n",
        "    actuals_list: list of n np.array one-hot vectors of shape (num_outputs,), each indicating\n",
        "                  the actual class at a particular target timestep (ordered from target timestep\n",
        "                  1 to target timestep n)\n",
        "    output_weights: np.array representing a vector of shape (num_hidden + 1,); represents \n",
        "                    the weights between the network's hidden layer and the output layer.\n",
        "                    The first element corresponds to the bias term.\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    values_list: list of n np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector \n",
        "                 of input values for a particular timestep (ordered from 1 to n).\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    hidden_list: a list of n np.arrays, each representing a vector of size (num_hidden,);\n",
        "                 represents the sequence of hidden layer activations, where each entry in \n",
        "                 the list is a vector of hidden layer activations for a particular timestep\n",
        "                 (ordered from 1 to n)\n",
        "    activation_derivative: function; a function that calculates the activation derivative\n",
        "                           of the hidden layer (default is derivative of ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple (input_weights_gradients_list, recurrent_weights_gradients_list, output_weights_gradients_list), \n",
        "    where:\n",
        "             input_weights_gradients_list: list of n np.arrays, each representing a matrix of size \n",
        "                                           (num_hidden, num_features + 1) which contains the gradient of the\n",
        "                                           loss function with respect to the input weights based on the error \n",
        "                                           at a particular target timestep T (accumulated over timesteps 1 to T).\n",
        "                                           The first element in each row corresponds to the bias term.\n",
        "                                           The matrices in the list are ordered from target timestep 1 to n.\n",
        "             recurrent_weights_gradients_list: list of n np.arrays, each representing a matrix of size \n",
        "                                               (num_hidden, num_hidden) which contains the gradient of the\n",
        "                                               loss function with respect to the recurrent weights based on the error \n",
        "                                               at a particular target timestep T (accumulated over timesteps 2 to T).\n",
        "                                               Does not contain a bias term.\n",
        "                                               The matrices in the list are ordered from target timestep 1 to n.\n",
        "             output_weights_gradients_list: list of n np.arrays, each representing a matrix of shape \n",
        "                                            (num_outputs, num_hidden + 1) which contains the gradient of the loss \n",
        "                                            function with respect to the output weights based on the error at a\n",
        "                                            particular timestep T (calculated based on timestep T).\n",
        "                                            The first element in each row corresponds to the bias term.\n",
        "                                            The matrices in the list are ordered from target timestep 1 to n.\n",
        "    \"\"\"  \n",
        "    # =====================================================\n",
        "    # DO NOT CHANGE THIS PART OR WRITE ABOVE IT\n",
        "    values_list = copy.deepcopy(values_list)\n",
        "    actuals_list = copy.deepcopy(actuals_list)\n",
        "    outputs_list = copy.copy(outputs_list)\n",
        "    hidden_list = copy.copy(hidden_list)\n",
        "    # =====================================================\n",
        "    \n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5055d3-e0d2-44be-998e-a87f52b2c452",
      "metadata": {
        "id": "0a5055d3-e0d2-44be-998e-a87f52b2c452"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8ab59b-7ec9-4b75-8643-537f461a67c8",
      "metadata": {
        "id": "6f8ab59b-7ec9-4b75-8643-537f461a67c8"
      },
      "outputs": [],
      "source": [
        "check(get_intermediate_weights_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e22634d-e463-4f43-ab37-b759748d64e6",
      "metadata": {
        "tags": [],
        "id": "3e22634d-e463-4f43-ab37-b759748d64e6"
      },
      "source": [
        "### Task 2.5.2: The complete backward pass [2 points]\n",
        "\n",
        "To calculate the overall weights gradients and complete the backward pass, we need to calculate the intermediate weights gradients ([RNN tagger backprop step 1](#RNN-TAGGER-BACKPROP-STEPS)) and then average them ([RNN tagger backprop step 2](#RNN-TAGGER-BACKPROP-STEPS)).\n",
        "\n",
        "Write a function `recurrent_tagger_backward()` that performs a complete backward pass ([RNN tagger backprop steps 1-2](#RNN-TAGGER-BACKPROP-STEPS)). Your function should take as arguments:  \n",
        "- a list `outputs_list` of $n$ output vectors (of shape `(num_outputs,)`), each representing the predicted probability distribution output by the softmax layer of the model at a particular target timestep (ordered from 1 to $n$);  \n",
        "- a list `actuals_list` of $n$ one-hot vectors (of shape `(num_outputs,)`), each representing the actual probabilities expected at the corresponding target timestep (where there is a 1 at the index of the actual class, and 0s elsewhere);  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first entry represents the bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a list `values_list` of the $n$ values vectors that are input to the model at each timestep (each a vector of shape `(num_features,)`, which does not contain a value for the bias term);  \n",
        "- a list `hidden_list` of the $n$ hidden layer activations at each of the timesteps (each a vector of shape `(num_hidden,)`; and  \n",
        "- an `activation_derivative` function (which is `relu_derivative` by default). \n",
        "\n",
        "It should return a **tuple** with *three* elements, where:  \n",
        "- the first element is the `input_weights_gradient` (including a bias term), averaged over the intermediate inputs weights gradients from timesteps 1 to $n$;  \n",
        "- the second element is the `recurrent_weights_gradient` (without a bias term), averaged over the intermediate recurrent weights gradients from timesteps 2 to $n$; and  \n",
        "- the third element is the `output_weights_gradient` (including a bias term), averaged over the intermediate output weights gradients from timesteps 1 to $n$.\n",
        "\n",
        "**Important:** be careful to exclude the `None` at the start of the `recurrent_weights_gradients_list`.\n",
        "\n",
        "*Hint: you should be able to reuse your `get_intermediate_weights_gradients()` function.*\n",
        "\n",
        "*Hint: you can use the `np.mean()` method to calculate the average of a list of arrays. To do so, provide the list of arrays as an argument, together with the keyword argument `axis=0`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd55d60f-f37b-4d5a-8ce2-41c058c0a4ba",
      "metadata": {
        "id": "fd55d60f-f37b-4d5a-8ce2-41c058c0a4ba"
      },
      "outputs": [],
      "source": [
        "def recurrent_tagger_backward(outputs_list, actuals_list, output_weights, \n",
        "                              recurrent_weights, values_list, hidden_list,\n",
        "                              activation_derivative=relu_derivative):\n",
        "    \"\"\"Calculates the overall weights gradients averaged over all timesteps in a 2-layer \n",
        "    recurrent neural network tagger.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    outputs_list: list of n np.arrays of shape (num_outputs,), each representing the probability\n",
        "                  distribution output by the tagger at a particular target timestep \n",
        "                  (ordered from target timestep 1 to target timestep n)\n",
        "    actuals_list: list of n np.array one-hot vectors of shape (num_outputs,), each indicating\n",
        "                  the actual class at a particular target timestep (ordered from target timestep\n",
        "                  1 to target timestep n)\n",
        "    output_weights: np.array representing a vector of shape (num_hidden + 1,); represents \n",
        "                    the weights between the network's hidden layer and the output layer.\n",
        "                    The first element corresponds to the bias term.\n",
        "    recurrent_weights: np.array representing a matrix of size (num_hidden, num_hidden);\n",
        "                       represents the weights between hidden layers of adjacent timesteps.\n",
        "                       Does not contain values for bias terms.\n",
        "    values_list: list of n np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector \n",
        "                 of input values for a particular timestep (ordered from 1 to n).\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    hidden_list: a list of n np.arrays, each representing a vector of size (num_hidden,);\n",
        "                 represents the sequence of hidden layer activations, where each entry in \n",
        "                 the list is a vector of hidden layer activations for a particular timestep\n",
        "                 (ordered from 1 to n)\n",
        "    activation_derivative: function; a function that calculates the activation derivative\n",
        "                           of the hidden layer (default is derivative of ReLU)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple (input_weights_gradient, recurrent_weights_gradient, output_weights_gradient), \n",
        "    where:\n",
        "             input_weights_gradient: np.array representing a matrix of size (num_hidden, num_features + 1) \n",
        "                                     which contains the gradient of the loss function with respect to the input \n",
        "                                     weights, based on the average error over all timesteps 1 to n.\n",
        "                                     The first element in each row corresponds to the bias term.\n",
        "             recurrent_weights_gradient: np.array representing a matrix of size (num_hidden, num_hidden) \n",
        "                                         which contains the gradient of the loss function with respect to the \n",
        "                                         recurrent weights, based on the average error over timesteps 2 to n.\n",
        "                                         Does not contain a value for the bias term.\n",
        "             output_weights_gradient: np.array representing a matrix of size (num_outputs, num_hidden + 1) \n",
        "                                      which contains the gradient of the loss function with respect to the output \n",
        "                                      weights, based on the average error over all timesteps 1 to n.\n",
        "                                      The first element in each row corresponds to the bias term.\n",
        "    \"\"\"  \n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba989cb7-1170-4958-a6c4-dbe9f03cc209",
      "metadata": {
        "id": "ba989cb7-1170-4958-a6c4-dbe9f03cc209"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e248ba2-2dea-4b7c-adc0-10b62a33a068",
      "metadata": {
        "id": "7e248ba2-2dea-4b7c-adc0-10b62a33a068"
      },
      "outputs": [],
      "source": [
        "check(recurrent_tagger_backward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31a68ac-aa57-46b5-bc00-f1c08de05c79",
      "metadata": {
        "tags": [],
        "id": "f31a68ac-aa57-46b5-bc00-f1c08de05c79"
      },
      "source": [
        "## Activity 2.6: Training a sequence tagger / RNN language model\n",
        "\n",
        "Training a sequence tagger via Stochastic Gradient Descent works just like training a sequence classifier, with the exception that having outputs at multiple timesteps means that the overall weights gradients now come via consideration of multiple intermediate weights gradients:  \n",
        "\n",
        "1. iterate over the training samples (in random order);  \n",
        "\n",
        "2. for each training sample consisting of input sequence $[\\mathbf{x}_1, \\cdots, \\mathbf{x}_n]$:  \n",
        "\n",
        "   1. complete a forward pass using the current input weights ($W$), recurrent weights ($U$), and output weights ($\\mathbf{v}^T$) to get the sequence of hidden layer activations $[\\mathbf{h}_1, \\cdots, \\mathbf{h}_n]$ and output probability distributions $[\\mathbf{p}_1, \\cdots, \\mathbf{p}_n]$;\n",
        "   \n",
        "   2. complete a backward pass that compares the sequence of outputs $[\\mathbf{p}_1, \\cdots, \\mathbf{p}_n]$ to the sequence of *actual* classes (one-hot vectors) for the training sample, and uses this comparison alongside the input ($[\\mathbf{x}_1, \\cdots, \\mathbf{x}_n]$) and hidden sequences ($[\\mathbf{h}_1, \\cdots, \\mathbf{h}_n]$) to derive overall weights gradients ($\\Delta W$, $\\Delta U$, $\\Delta V$);  \n",
        "      \n",
        "   3. update each set of weights by *subtracting* a nudge based on the corresponding overall weights gradient and the learning rate $\\eta$ *(NB: $\\leftarrow$ means \"is overwritten by\")*:  \n",
        "   \n",
        "      $$W \\leftarrow W - \\Delta W \\times \\eta$$  \n",
        "      $$U \\leftarrow U - \\Delta U \\times \\eta$$  \n",
        "      $$V \\leftarrow V - \\Delta V \\times \\eta$$  \n",
        "      \n",
        "3. repeat for a fixed number of epochs (entire cycles through the training data), or until convergence  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a609d9a-9684-47a4-aa07-52a281a4b988",
      "metadata": {
        "tags": [],
        "id": "6a609d9a-9684-47a4-aa07-52a281a4b988"
      },
      "source": [
        "### Task 2.6.1: Training for a single epoch [3 points]\n",
        "\n",
        "Write a function `train_epoch_tagger()` that performs steps 1-2 of the training outline above. Your function should takes as arguments:  \n",
        "- a list of (`values_list`, `actuals_list`) pairs representing training samples, where:  \n",
        "  - `values_list` is a list of the $n$ values vectors that are input to the model at each timestep, arranged in order from timestep 1 to $n$ (each a vector of shape `(num_features,)`, which does not contain a value for the bias term); and  \n",
        "  - `actuals_list` is a list of $n$ one-hot vectors (of shape `(num_outputs,)`), each representing the actual probabilities expected at the corresponding target timestep (where there is a 1 at the index of the actual class, and 0s elsewhere);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a vector of `output_weights` (of shape `(num_hidden + 1,)`, where the first element corresponds to a bias term);  \n",
        "- a `learning_rate`; and  \n",
        "- the **name** of an activation function (a **string**, which is `\"relu\"` by default). \n",
        "\n",
        "It should iterate over the training samples, nudging the weights for each one, and then return the final weights after nudging once for every training sample.\n",
        "\n",
        "**Note:** We have provided code that uses the name you provide for an activation function (e.g. `\"relu\"`) to look up corresponding *functions* to use for hidden layer activations and derivatives (e.g. `relu()` and `relu_derivative()`). These functions are stored in the variables `activation_function` and `activation_derivative`, respectively; make sure that you pass these variables as arguments to `recurrent_tagger_forward()` and `recurrent_tagger_backward()`, as in `recurrent_tagegr_forward(..., activation_function=activation_function)` and `recurrent_tagger_backward(..., activation_derivative=activation_derivative)`. We have also taken a *copy* of some of the arguments at the beginning of the code, to prevent them being changed due to mutability issues. This is important for being able to re-use functions later on.\n",
        "\n",
        "**Note:** You may assume that the training samples have already been shuffled into a random order.\n",
        "\n",
        "*Hint: Make sure that your nudges stack up, by overwriting the weights with their new values after each training sample.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4fd90e-b26d-439d-bb59-7d10b458474a",
      "metadata": {
        "id": "1c4fd90e-b26d-439d-bb59-7d10b458474a"
      },
      "outputs": [],
      "source": [
        "def train_epoch_tagger(train_samples, input_weights, recurrent_weights, \n",
        "                       output_weights, learning_rate, activation_name=\"relu\"):\n",
        "    \"\"\"Applies a single epoch of SGD to a RNN sequence tagger, and returns \n",
        "    the resultant new weights.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    train_samples: list of pairs (values_list, actuals_list), where each pair\n",
        "                   corresponds to a single training sample for which:\n",
        "                   values_list is a list of input values vectors (without bias\n",
        "                               term), one for each timestep; and\n",
        "                   actuals_list is a list of actual class one-hot vectors, one\n",
        "                                for each timestep\n",
        "    input_weights: np.array of weights between the input and hidden layer, \n",
        "                   including a weight for the bias term.\n",
        "    recurrent_weights: np.array of weights between hidden layers at successive\n",
        "                       timepoints, with no weight for the bias term.\n",
        "    output_weights: np.array of weights between the final hidden layer and the\n",
        "                    output, including a weight for the bias term.\n",
        "    learning_rate: float; the learning rate, eta\n",
        "    activation_name: str; the name of an activation function, for which there is\n",
        "                     a corresponding activation derivative function with the suffix\n",
        "                     _derivative\n",
        "                     \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple (new_input_weights, new_recurrent_weights, new_output_weights),\n",
        "             where each set of weights has had a single nudge update for each\n",
        "             training sample\n",
        "    \"\"\"\n",
        "    # ===========================================================\n",
        "    # DO NOT CHANGE THIS PART OR WRITE ABOVE IT\n",
        "    activation_function = eval(activation_name)\n",
        "    activation_derivative = eval(activation_name + \"_derivative\")\n",
        "    input_weights = copy.copy(input_weights)\n",
        "    recurrent_weights = copy.copy(recurrent_weights)\n",
        "    output_weights = copy.copy(output_weights)\n",
        "    # ===========================================================\n",
        "    \n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294b5b7f-d3eb-43e2-8a31-94abd17496ec",
      "metadata": {
        "id": "294b5b7f-d3eb-43e2-8a31-94abd17496ec"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f615a8-c01d-4e6e-8adb-aaf9a7822083",
      "metadata": {
        "id": "46f615a8-c01d-4e6e-8adb-aaf9a7822083"
      },
      "outputs": [],
      "source": [
        "check(train_epoch_tagger)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c9bc39-7628-4551-8c85-963091dad75e",
      "metadata": {
        "id": "97c9bc39-7628-4551-8c85-963091dad75e"
      },
      "source": [
        "### Task 2.6.2: Calculating the loss of a single sample [3 points]\n",
        "\n",
        "To assess how the model is performing, we need to be able to calculate the loss over a set of (training or test) samples. Here, we will consider the loss of a single sample.\n",
        "\n",
        "We use *time-averaged cross-entropy loss*. This loss is calculated slightly differently than the binary cross-entropy loss we saw in the RNN sequence classifier we saw above, for two reasons:  \n",
        "1. the classifications that the network is doing are now *multi-choice*, not binary; and  \n",
        "2. there are *multiple* classifications for a single input sequence (one per timestep).\n",
        "\n",
        "We account for these differences as follows:\n",
        "\n",
        "**Difference 1**\n",
        "\n",
        "For a *multi-choice* classification, the network outputs a *vector* of probabilities, $\\mathbf{p}$. We have a corresponding one-hot vector of actual probabilities we wanted the network to output, $\\mathbf{a}$. The cross-entropy loss associated with this output comes from considering *only the probability of the actual class*, and taking the negative log of this probability. That is,\n",
        "\n",
        "  $$L = -\\log{p_C} \\quad \\text{where } C \\text{ is the actual class}$$\n",
        "\n",
        "You'll often see the cross-entropy loss written as:\n",
        "\n",
        "  $$L = \\sum_i{\\left[a_i \\times \\left(-\\log{p_i}\\right)\\right]}$$\n",
        "  \n",
        "This is equivalent to the version given above, because if $i \\neq C$ then $a_i=0$, so that $\\left[a_i \\times \\left(-\\log{p_i}\\right)\\right] = \\left[0 \\times \\left(-\\log{p_i}\\right)\\right] = 0$.  \n",
        "Conversely, $a_C=1$ and thus $\\left[a_C \\times \\left(-\\log{p_C}\\right)\\right] = \\left[1 \\times \\left(-\\log{p_C}\\right)\\right] = -\\log{p_C}$.\n",
        "\n",
        "This can be further written as a dot product,\n",
        "\n",
        "  $$L = \\mathbf{a}^T \\mathbf{n} \\quad \\text{where } \\mathbf{n} = -\\log{\\mathbf{p}}$$\n",
        "\n",
        "**Difference 2**\n",
        "\n",
        "For a situation where there are multiple classifications for each input sequence (difference 2), the overall loss for the input sequence is the *average* of the losses for each classification. That is, for an output $\\mathbf{p}_t$ at a given timestep, we calculate its loss as above, and then we average the losses across timesteps. This ensures that the loss associated with a sequence is not based primarily on its length.\n",
        "\n",
        "**Your task**\n",
        "\n",
        "Write a function `tagged_sample_loss()` that calculates the time-averaged cross-entropy loss of a recurrent neural network tagger for a single (training or test) sample. Your function should take as arguments:  \n",
        "- a list `values_list` of the $n$ values vectors that are input to the model at each timestep, arranged in order from timestep 1 to $n$ (each a vector of shape `(num_features,)`, which does not contain a value for the bias term);  \n",
        "- a list `actuals_list` of $n$ one-hot vectors (of shape `(num_outputs,)`), each representing the actual probabilities expected at the corresponding target timestep (where there is a 1 at the index of the actual class, and 0s elsewhere);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element in each row corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a matrix of `output_weights` (of shape `(num_outputs, num_hidden + 1)`, where the first element in each row corresponds to a bias term); and  \n",
        "- an `activation_function` (which is `relu` by default).\n",
        "\n",
        "It should return a float representing the time-averaged cross-entropy loss for the sample.\n",
        "\n",
        "**Important:** `tagged_sample_loss()` also has an argument `eps`, which is set to a small value. You should use `eps` to adjust the value you are taking the log of when calculating the loss for a sample: if this value is lower than `eps`, you should instead replace it by `eps` prior to taking the log. This prevents numerical instability caused by the fact that `log(0)` is undefined. *Hint: the `np.maximum()` method might be helpful here.*\n",
        "\n",
        "*Note: you should use the function `np.log()` to take logs.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1e69f5-8f5a-484c-ac46-b08a11a5896d",
      "metadata": {
        "id": "3f1e69f5-8f5a-484c-ac46-b08a11a5896d"
      },
      "outputs": [],
      "source": [
        "def tagged_sample_loss(values_list, actuals_list, input_weights, recurrent_weights, \n",
        "                       output_weights, activation_function=relu, eps=1e-8):\n",
        "    \"\"\"Calculates the time-averages cross-entropy loss of a RNN tagger for a given sample.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    values_list: list of n np.arrays, each representing a vector of size (num_features,);\n",
        "                 represents the input sequence, where each entry in the list is a vector \n",
        "                 of input values for a particular timestep (ordered from 1 to n).\n",
        "                 None of these vectors contain a value for the bias term.\n",
        "    actuals_list: list of n np.array one-hot vectors of shape (num_outputs,), each indicating\n",
        "                  the actual class at a particular target timestep (ordered from target timestep\n",
        "                  1 to target timestep n)\n",
        "    input_weights: np.array of weights between the input and hidden layer, \n",
        "                   including a weight for the bias term.\n",
        "    recurrent_weights: np.array of weights between hidden layers at successive\n",
        "                       timepoints, with no weight for the bias term.\n",
        "    output_weights: np.array of weights between the final hidden layer and the\n",
        "                    output, including a weight for the bias term.\n",
        "    activation_function: function; the activation function for the hidden layer\n",
        "                         (default is ReLU)\n",
        "    eps: float; the minimum value that should be used as argument to np.log(),\n",
        "         to prevent numerical instabilities.\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a55dd5-bbe3-4df7-9d69-95f5dc6bd359",
      "metadata": {
        "id": "f9a55dd5-bbe3-4df7-9d69-95f5dc6bd359"
      },
      "source": [
        "To make sure that your function incorporates the `eps` argument correctly, run the following cell. If you haven't incorporated `eps` correctly, your function will return `inf` (and you might see a warning message). If you have incorporated `eps` correctly, your function will return ~18.4207."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159e8dff-93a8-4b31-b92f-6a5a9a210196",
      "metadata": {
        "tags": [],
        "id": "159e8dff-93a8-4b31-b92f-6a5a9a210196"
      },
      "outputs": [],
      "source": [
        "values_list = [np.array([0, 1]), np.array([1, 0])]\n",
        "actuals_list = [np.array([1, 0], dtype=\"float64\"), np.array([0, 1], dtype=\"float64\")]\n",
        "input_weights = np.array([[0, 1, 0], [0, 0, 1]], dtype=\"float64\")\n",
        "recurrent_weights = np.array([[1, 1], [1, 1]], dtype=\"float64\")\n",
        "output_weights = np.array([[0, 100, 0], [0, 0, 100]], dtype=\"float64\")\n",
        "\n",
        "tagged_sample_loss(values_list, actuals_list, input_weights, recurrent_weights, output_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d93a855c-9f3c-496a-bf15-9e056448be3a",
      "metadata": {
        "id": "d93a855c-9f3c-496a-bf15-9e056448be3a"
      },
      "source": [
        "To test your function, run the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5decd019-1f94-4b76-9613-3438b985a020",
      "metadata": {
        "id": "5decd019-1f94-4b76-9613-3438b985a020"
      },
      "outputs": [],
      "source": [
        "check(tagged_sample_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d01ca299-86b8-4411-8605-a99d9b39f0a6",
      "metadata": {
        "tags": [],
        "id": "d01ca299-86b8-4411-8605-a99d9b39f0a6"
      },
      "source": [
        "### Task 2.6.3: Tracking the loss [3 points]\n",
        "\n",
        "As always, the overall loss of the network on a given set of (training or test) samples is the average of the losses for each individual sample.\n",
        "\n",
        "Write a function `recurrent_tagger_loss()` that calculates the overall cross-entropy loss of a recurrent neural network classifier for a given set of (training or test) samples. Your function should take as arguments:  \n",
        "- a list of `samples` structured as (`values_list`, `actuals_list`) pairs, where:  \n",
        "  - `values_list` is a list of the $n$ values vectors that are input to the model at each timestep, arranged in order from timestep 1 to $n$ (each a vector of shape `(num_features,)`, which does not contain a value for the bias term); and  \n",
        "  - `actuals_list` is a list of $n$ one-hot vectors (of shape `(num_outputs,)`), each representing the actual probabilities expected at the corresponding target timestep (where there is a 1 at the index of the actual class, and 0s elsewhere);  \n",
        "- a matrix of `input_weights` (of shape `(num_hidden, num_features + 1)`, where the first element in each row corresponds to a bias term);  \n",
        "- a matrix of `recurrent_weights` (of shape `(num_hidden, num_hidden)`);  \n",
        "- a matrix of `output_weights` (of shape `(num_outputs, num_hidden + 1)`, where the first element in each row corresponds to a bias term); and  \n",
        "- an `activation_function` (which is `relu` by default).\n",
        "\n",
        "It should return a float representing the average of the losses for each individual sample.\n",
        "\n",
        "*Hint: you should use your `tagged_sample_loss()` function to calculate the loss of each sample, and then average these losses.*\n",
        "\n",
        "**Important:** `recurrent_tagger_loss()` also has an argument `eps`, which is set to a small value. You should pass `eps` to your `tagged_sample_loss()` function, as in `tagged_sample_loss(..., eps=eps)`. You should also pass on the `activation_function` as in previous functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "765de70f-2cb7-4c7c-8f65-d59a79ac61b4",
      "metadata": {
        "id": "765de70f-2cb7-4c7c-8f65-d59a79ac61b4"
      },
      "outputs": [],
      "source": [
        "def recurrent_tagger_loss(samples, input_weights, recurrent_weights, \n",
        "                          output_weights, activation_function=relu, eps=1e-8):\n",
        "    \"\"\"Calculates the time-averages cross-entropy loss of a RNN tagger for a given sample.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    samples: list of pairs (values_list, actuals_list), where each pair\n",
        "             corresponds to a single sample for which:\n",
        "             values_list is a list of input values vectors (without bias\n",
        "                         term), one for each timestep; and\n",
        "             actuals_list is a list of actual class one-hot vectors, one\n",
        "                          for each timestep\n",
        "    input_weights: np.array of weights between the input and hidden layer, \n",
        "                   including a weight for the bias term.\n",
        "    recurrent_weights: np.array of weights between hidden layers at successive\n",
        "                       timepoints, with no weight for the bias term.\n",
        "    output_weights: np.array of weights between the final hidden layer and the\n",
        "                    output, including a weight for the bias term.\n",
        "    activation_function: function; the activation function for the hidden layer\n",
        "                         (default is ReLU)\n",
        "    eps: float; the minimum value that should be used as argument to np.log(),\n",
        "         to prevent numerical instabilities.\n",
        "    \"\"\"\n",
        "    # TODO: write this function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb444d3-f979-414a-8df8-5aea6c9ccab5",
      "metadata": {
        "id": "ccb444d3-f979-414a-8df8-5aea6c9ccab5"
      },
      "source": [
        "To test your code, run the cell below. You should see the loss decrease from ~1.233 with original weights to ~1.032 after 1 epoch. This means training is helping!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f66d0a-b9f5-4aa1-84b0-cc5fdb8361f8",
      "metadata": {
        "id": "e8f66d0a-b9f5-4aa1-84b0-cc5fdb8361f8"
      },
      "outputs": [],
      "source": [
        "train_samples = [\n",
        "    ([np.array([1, 2, 3], dtype=\"float64\"),\n",
        "      np.array([0, 1, 1], dtype=\"float64\")],\n",
        "     [np.array([0, 1, 0], dtype=\"float64\"),\n",
        "      np.array([1, 0, 0], dtype=\"float64\")]),\n",
        "    ([np.array([1, 1, 0], dtype=\"float64\"),\n",
        "      np.array([2, 0, 1], dtype=\"float64\")],\n",
        "     [np.array([0, 1, 0], dtype=\"float64\"),\n",
        "      np.array([0, 0, 1], dtype=\"float64\")]),\n",
        "    ([np.array([1, 2, 0], dtype=\"float64\"),\n",
        "      np.array([-1, 1, 0], dtype=\"float64\")],\n",
        "     [np.array([1, 0, 0], dtype=\"float64\"),\n",
        "      np.array([1, 0, 0], dtype=\"float64\")])\n",
        "]\n",
        "input_weights = np.array([[-1, 1, 0, 0], [-1, 0, 1, 0]], dtype=\"float64\")\n",
        "recurrent_weights = np.array([[0, 1], [-1, 1]], dtype=\"float64\")\n",
        "output_weights = np.array([[1, 1, -1], [np.log(2), 0, 0], [0, 1, 0]], dtype=\"float64\")\n",
        "learning_rate = 0.1\n",
        "\n",
        "trained_weights = train_epoch_tagger(train_samples, input_weights, recurrent_weights, \n",
        "                                     output_weights, learning_rate)\n",
        "\n",
        "untrained_loss = recurrent_tagger_loss(train_samples, input_weights, recurrent_weights, output_weights)\n",
        "print(\"Loss with original weights: {}\".format(untrained_loss))\n",
        "\n",
        "trained_loss = recurrent_tagger_loss(train_samples, *trained_weights)\n",
        "print(\"Loss after 1 epoch: {}\".format(trained_loss))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:ling111] *",
      "language": "python",
      "name": "conda-env-ling111-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false,
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}